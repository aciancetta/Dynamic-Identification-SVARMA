[["index.html", "Dynamic Identification in SVARMA Models Abstract", " Dynamic Identification in SVARMA Models Alessandro Ciancetta 2021-07-13 Abstract This book present the general framework of VARMA models and introduces the notion of non-fundamentalness. The problem is assessed using the methodology of Gourieroux, Manfort and Renne (2019), which is illustrated in detail, within the framework of non-Gaussian and independent structural shocks of the corresponding structural model. Simulations and application to real-world data are presented. "],["introduction.html", "Section 1 Introduction 1.1 Assumptions for SSVARMA models 1.2 Invertibility of the MA part 1.3 Non-fundamentalness", " Section 1 Introduction VARMA models are an interesting generalization of ARMA models for multivariate time series. This class of models take into account the (direct) lagged effect that a shock can have on the system under investigation. This could be useful in practice, for example whenever we think that the process might react to the shocks also after some delay (i.e. if the error term has an MA structure). However, VARMA models are not as popular as standard VAR models, also because of the difficulties of estimating these models when compared with the simple least-squares estimation which is common in the VAR literature. The features of these models, however, may reveal crucial also for structural-VAR modeling. In particular, a VAR model can be seen as an approximation of an underlying VARMA process, but only if certain conditions on the moving-average parameters hold. Basically these conditions states that the parameters of the delayed effect of the shock should be smaller than one in modulus: that is, the shocks should not have an explosive lagged impact on the system. This condition - which we will make more precise throughout this essay - allows for the invertibility of the MA part of the model, thus making VAR approximations possible. However, in practice one does not know a priori whether the condition hold or not, i.e. whether the shocks are respectively fundamental or not-fundamental. In the latter case, the process cannot be inverted in its past values and an autoregressive representation of the model does not exist: a VAR representation, since it fails to identify the true parameters, would then be wrong, and the estimated impulse response functions would not be those associated with the true non-fundamental process. Therefore, if we have some reasons to believe that the process that we are estimating has a VARMA structure, we should keep in mind that VAR modeling could be misleading. In this essay, we discuss the methodology proposed by Gourieroux, Monfort and Renne (2019, GMR2019 henceforth) to assess the problem of dynamic identification of non-fundamental SVARMA processes. Section 1 presents the main assumptions and provides a formal definition of non-fundamentalness. Section 2 presents the method in detail. Section 3 discusses a simple simulation example in the case of a bivariate model. Section 4 is a toy-example of applications to real-world data. 1.1 Assumptions for SSVARMA models Consider the following VARMA model: \\[ \\Phi(L) Y_t = \\Theta(L) \\varepsilon_t, \\quad Y_t \\in \\mathbb{R}^n \\] where \\[ \\Phi(L) = I_n - \\Phi_1L - \\dots - \\Phi_p L^p \\] \\[ \\Theta(L) = I_n - \\Theta_1L - \\dots - \\Theta_q L^q \\] We want to make the following assumptions: Hp. 1 (Independent shocks). There exists a linear transformation such that the (correlated) VARMA residuals can be expressed as the transformation of a vector of structural shocks that are mutually independent, that is: \\[ \\exists \\ C \\in M_{n\\times n} \\ | \\ \\varepsilon_t = C \\eta_t \\quad \\text{with } \\det C \\neq 0, \\] where \\(\\eta_t \\in \\mathbb{R}^n\\) is such that \\((\\eta_1, \\eta_2, \\dots , \\eta_T)\\) are independently and identically distributed (so that the shocks at each time period are random draws from the same joint distribution) and \\((\\eta_{1t}, \\eta_{2t}, \\dots, \\eta_{nt})\\) are mutually independent (i.e. the joint distribution of the shocks is the product of their marginal distributions). \\(\\eta_t\\) are called the structural shocks of the process. The condition \\(\\det C \\neq 0\\) means that the transformation has full rank, and therefore that both \\(\\varepsilon_t\\) and \\(\\eta_t\\) belongs to \\(\\mathbb{R}^n\\) and the number of structural shocks equals the number of variables in the system. Also, it implies the existence of \\(C^{-1}\\). When Hp.1 holds, the process is said to be a strong-structural VARMA model (SSVARMA), where strong derives from the assumption of mutual independence of the simultaneous shocks. Hp. 2 (Minimum order representation). Matrices \\(\\Phi(L), \\ \\Theta(L)\\) are left-comprime, i.e. if there exists a common matrix \\(C(L)\\) such that \\[ \\begin{aligned} \\Phi(L) = C(L) \\widetilde{\\Phi}(L) \\\\ \\Theta(L) = C(L) \\widetilde{\\Theta}(L) \\end{aligned} \\qquad \\text{then } \\det C(L) \\text{ does not depend on L} \\] Since \\(\\det \\Phi(L) = \\det C(L) \\ \\det \\widetilde{\\Phi}(L)\\) and \\(C(L)\\) does not depend on L, Hp.2 implies that \\(\\det \\Phi(L)\\) and \\(\\det \\widetilde{\\Phi}(L)\\) have the same lag order (the same holds for \\(\\det \\Theta(L)\\) and \\(\\det \\widetilde{\\Theta}(L)\\)). Therefore, simplifying \\[ C(L) \\widetilde{\\Phi}(L) = C(L) \\widetilde{\\Theta}(L) \\varepsilon_t \\] by left-multiplying \\(C(L)^{-1}\\) on both sides of the equation would not lead to a reduction of the lag order. This means that both the AR and MA components have minimal order. Hp.3 (Stationarity of the AR component). \\(\\det \\Phi (z) \\neq 0, \\ \\forall z \\in \\mathbb{C} \\ \\text{s.t.} \\ |z| &lt; 1\\), i.e. the determinantal polynomial has roots outside the unit circle. To understand this condition, we have to work on the relationship between the roots of \\(\\Theta(z)\\) and the eigenvalues of the companion matrix. Indeed, stationarity is guaranteed when the eigenvalues of the companion matrix are inside the unite circle. A VARMA(p,q) process has the following VARMA(1, q) representation: \\[ \\mathbf{y_t} = \\mathbf{F y_{t-1}} + \\mathbf{v_t} \\] with \\[\\begin{equation} \\mathbf{F} =\\begin{bmatrix} \\Phi_1 &amp; \\Phi_2 &amp; \\dots &amp; \\Phi_{p-1} &amp; \\Phi_p \\\\ I_n &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; I_n &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; I_n &amp; 0 \\end{bmatrix}, \\qquad \\mathbf{y_t} =\\begin{bmatrix} Y_t \\\\ Y_{t-1} \\\\ \\vdots \\\\ Y_{t-p} \\end{bmatrix}, \\qquad \\mathbf{v_t} =\\begin{bmatrix} \\Theta(L)\\varepsilon_t \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\end{equation}\\] Therefore, we can obtain the expression of the \\(h\\)-steps ahead observation as \\[ \\mathbf{y_{t+h}} = \\mathbf{v_{t+h}} + \\mathbf{F v_{t+h-1}} + \\mathbf{F^2 v_{t+h-2}} + \\dots + \\mathbf{F^{h-1} v_{t+1}} + \\mathbf{F^h y_{t}}, \\] from which it is readily seen that, in order for the process to be stationary, the eigenvalues of \\(\\mathbf{F}\\) must lay inside the unit circle (i.e. the effect of each \\(\\mathbf{v_t}\\) must die out). According to Proposition 10.1 in Hamilton (1994), the eigenvalues of \\(\\mathbf{F}\\) satisfy \\[\\begin{equation} \\det(I_n \\lambda^p - \\Phi_1\\lambda^{p-1} - \\dots - \\Phi_{p-1}\\lambda - \\Phi_p) = 0 \\tag{1.1} \\end{equation}\\] We can show that the statement in Hp.3 is just an alternative writing for: the solutions of \\(\\det(I_n \\lambda^p - \\Phi_1\\lambda^{p-1} - \\dots - \\Phi_{p-1}\\lambda - \\Phi_p) = 0\\) (i.e. the eigenvalues of \\(\\mathbf{F}\\)) must be smaller than one in modulus. Indeed, we can factor the polynomial operator \\(\\Phi(L)\\) as: \\[ I - \\Phi_1L - \\Phi_2L^2 - \\dots - \\Phi_p L^p = (I-\\lambda_1 I L) (I-\\lambda_2 I L) \\cdots(I-\\lambda_p I L) \\] The two sides of the previous equation are equal if and only if the following equality holds \\(\\forall z \\in \\mathbb{C}\\): \\[ I - \\Phi_1 z - \\Phi_2 z^2 - \\dots - \\Phi_p z^p = (I-\\lambda_1 I z) (I-\\lambda_2 I z) \\cdots(I-\\lambda_p I z) \\] We can divide both sides by \\(z^p\\) and take the determinant to obtain an expression similar to eq. (1.1) on the left-hand side: \\[\\begin{equation} \\det(I z^{-p} - \\Phi_1 z^{-p+1} - \\dots - \\Phi_{p-1} z^{-1} - \\Phi_p) \\tag{1.2} \\end{equation}\\] Polynomials in (1.1) and (1.2) are equal if and only their roots are \\(z_i = \\lambda_i^{-1}, \\ \\ i \\in \\{1, \\dots, p\\}\\). This entails that the eigenvalues of \\(\\mathbf{F}\\) are inside the unit circle if and only if the roots of the polynomial (1.2) are outside the unit circle. Basically, this equivalence comes from the different representations of the VARMA process as a difference equation or as an equation in the lag operator. Therefore, the condition in Hp.3 is a necessary and sufficient condition for the stationarity of the autoregressive part of the process. Hp. 4 (No unit roots in the MA part). \\(\\det \\Theta(z) \\neq 0, \\ \\forall z \\in \\mathbb{C} \\text{ s.t } |z| = 1\\). This condition: guarantees the generalized invertibility of the MA part, in the following sense: \\(\\exists D(L) = \\sum_{k = -\\infty}^{+\\infty} D_k L^k\\) s.t. \\(D(L) \\Theta(L) = I\\), and therefore \\(\\eta_t = C^{-1}D(L)\\Phi(L)Y_t\\). This means that the structural shocks can be retrieved when all the past, current and future values of \\(Y_t\\) are observed. rules out cointegration (Hamilton 1994, p. 575). Indeed, consider the vector moving average (VMA) representation of the VARMA process, whose existence is guaranteed by Hp. 3: \\[ Y_t = \\underbrace{\\Phi(L)^{-1} \\Theta(L)}_{\\Psi(L)} \\varepsilon_t \\] A necessary condition for cointegration is the existence of a matrix \\(A\\in M_{n \\times n}\\) such that \\(A&#39; \\ \\Psi(1) = 0\\). This in turn implies that \\(\\det \\Psi(z) = \\det \\Phi(z)^{-1} \\det \\Theta(z) = 0\\) at \\(z \\in \\mathbb{C} \\text{ s.t. } |z| = 1\\). Since by Hp. 3\\(\\det \\Phi(z)^{-1} \\neq 0, \\ \\forall z \\in \\mathbb{C} \\ s.t. |z| \\geq 1\\), this implies that \\(\\det \\Theta(z)\\) has a unit root. Therefore, Hp. 4 is a sufficient condition to rule out the cointegration of the VARMA process. 1.2 Invertibility of the MA part A common approach in the estimation of VARMA models is to consider the truncated VAR representation of the process: \\(A(L) Y_t = \\varepsilon_t\\), where \\(A(L) = \\Theta(L)^{-1} \\Phi(L)\\). However, an implicit assumption behind this procedure is that \\(\\Theta(L)\\) is invertible in the past, i.e. \\(\\det\\Theta(z) \\neq 0, \\ \\forall z \\in \\mathbb{C} \\ s.t. \\ |z| &lt; 1\\). The following example illustrates the problem of ill-located roots of the determinental polynomial. Example 1.1 (Ill-located MA roots) Consider an ARMA(1,1) process: \\[ (1-\\phi L) y_t = (1 + \\theta L) \\varepsilon_t, \\quad |\\phi| &lt; 1, \\ |\\theta| &gt;1. \\] Since the AR component of the process is invertible, it admits an MA(\\(\\infty\\)) representation: \\[ \\begin{align}y_t &amp;= (1-\\phi L)^{-1}(1 + \\theta L) \\varepsilon_t = \\left(\\sum_{k=0}^{\\infty} \\phi^k L^k\\right) (1 + \\theta L) \\varepsilon_t = \\\\&amp;= \\sum_{k=0}^{\\infty} \\phi^k \\varepsilon_{t-k} + \\theta \\underbrace{\\sum_{k=1}^{\\infty} \\phi^k \\varepsilon_{t-k}}_{\\sum_{k=0}^{\\infty} \\phi^k \\varepsilon_{t-k}-\\varepsilon_t} \\\\\\implies y_t &amp;= (1+\\theta) \\sum_{k=0}^{\\infty} \\phi^k \\varepsilon_{t-k} - \\theta \\varepsilon_t\\end{align} \\]That is, the value observed at time \\(t\\) can be expressed as a linear combination of the present and past values of the shocks. On the contrary, the process does not admit an AR(\\(\\infty\\)) representation. Indeed, let \\(\\theta(L) = 1 + \\theta L\\). The equality holds if and only if it is \\(\\theta(z) = 1 + \\theta z\\), \\(\\forall z \\in \\mathbb{C}\\). The root of this polynomial is \\(z = -1/\\theta \\in \\mathbb{R}\\), which is smaller than one in modulus. This implies that the eigenvalue of the companion matrix \\(\\lambda\\) is such that \\(|\\lambda| &gt;1\\) (indeed, the companion matrix is the scalar \\(\\theta\\) itself). Therefore, the shock \\(\\varepsilon_t\\) would be an infinite sum of terms greater than one, which does not converge to any finite number. However, it is possible to obtain an alternative representation, which is autoregressive in the future values of \\(y_t\\). Start by rewriting \\(1+\\theta L\\) as \\(\\theta L (1 + \\frac{1}{\\theta} L^{-1})\\), where \\(L^{-1}\\) is the forward operator. The operator \\(1 + \\frac{1}{\\theta} L^{-1}\\) is invertible, and the process can be rewritten as: \\[ \\begin{align}\\theta L \\varepsilon_t &amp;= (1 + \\theta ^{-1} L ^{-1}) (1 - \\phi L ) y_t = \\\\&amp;= \\left( \\sum_{k = 0}^{\\infty} \\theta ^{-k} L ^{-k}\\right) (1-\\phi L) y_t \\end{align} \\] which, after some manipulations, becomes \\[ \\varepsilon_t = (1-\\phi)\\sum_{k = 1}^\\infty \\frac{1}{\\theta^k}y_{t+k} - \\phi y_t. \\] That is, the shocks of the process can be rewritten as a linear combination of the present and future values of \\(y_t\\). 1.3 Non-fundamentalness As the example shows, when the MA polynomial has some roots inside the unit circle there is an asymmetry in the information contained in the structural shocks and in the observed values. In this case, if we knew all the present and past shocks of the process we could retrieve its present and past values. On the contrary, if we knew all the present and past values of the process, we would still be unable to retrieve the past and present shocks. We can provide a rigorous explanation of this fact through the following Definition 1.1 Consider the process \\(x_t \\in \\mathbb{R}^n\\). Given its present and past observations\\(\\{x_t, x_{t-1}, x_{t-2},\\dots\\}\\), the information set is defined as \\[ \\mathcal{H}^t_x = \\overline{\\text{span}}(x_t, x_{t-1}, x_{t-2}, \\dots), \\] i.e. it is the set of all the possible linear combinations of the present and past values of \\(x_t\\). From the previous example, we can notice that: Knowing \\(\\{\\varepsilon_{t}, \\varepsilon_{t-1}, \\varepsilon_{t-2}, \\dots \\}\\) implies knowing also \\(\\{y_t, y_{t-1}, y_{t-2}, \\dots\\}\\) \\(\\implies\\) \\(\\mathcal{H}_y \\subseteq \\mathcal{H}_{\\varepsilon}\\) Knowing \\(\\{y_t, y_{t-1}, y_{t-2}, \\dots\\}\\) does not imply knowing also \\(\\{\\varepsilon_{t}, \\varepsilon_{t-1}, \\varepsilon_{t-2}, \\dots \\}\\) \\(\\implies\\) \\(\\mathcal{H}_\\varepsilon \\not\\subseteq \\mathcal{H}_{y}\\) Therefore, \\(\\mathcal{H}_y \\subset \\mathcal{H}_{\\varepsilon}\\). When this is the case, we say that the VARMA process is non-fundamental. Definition 1.2 The VARMA process \\(\\Phi(L) Y_t = \\Theta(L) \\varepsilon_t\\) is said to be non-fundamental when \\(\\mathcal{H}_Y \\subset \\mathcal{H}_{\\varepsilon}\\), or, analogously, when \\(\\exists z \\in \\mathbb{C}, |z|&lt;1, \\ s.t. \\ \\det(\\Theta(z)) = 0.\\) In that case, there exists at least one eigenvalue of the companion matrix of the MA part of the process that is larger than one in modulus. Non-fundamentalness may be a problem for VAR modeling in many practical situations. Indeed, if the VAR model approximates an underlying VARMA model, the identification of the true coefficients is impossible whenever the effect of some shocks intensifies after one or more time periods. Namely, non-fundamentalness arises whenever some entries of the \\(\\Theta\\) matrices are too big (see for instance Figure 6 in GMR2019 for the 2x2 case). This could be a relevant case in many economic application, where the strength of a shock could well increase after one or more period. Indeed, after its first outbreak, a shock might propagate in the system and, by affecting the expectations of the agents, it could reach its maximum effect with some delay. Therefore, the case of eigenvalues of the MA companion matrix greater than one in modulus cannot be simply discarded. The problem is made more significant by the fact that it is not possible to distinguish between a fundamental and a non-fundamental process using the standard techniques. This can be shown by exploiting the following relevant corollary to Theorem 2 in Lippi and Reichlin (1994): Theorem 1.1 Any non-fundamental VARMA process admits a fundamental representation that can be obtained by using Blaschke matrices. That is, given a non-fundamental \\(\\Phi(L) Y_t = \\Theta(L)\\varepsilon_t\\), there exists an invertible matrix \\(B(L)\\) with the property that \\(B(L)^{-1} = B^*(L^{-1})\\) such that \\[ \\Phi(L)Y_t = \\underbrace{\\Theta(L)B(L)^{-1}}_{\\tilde{\\Theta}(L)} \\underbrace{B(L)\\varepsilon_t}_{\\tilde{\\varepsilon}_t} \\] is fundamental, i.e. \\(\\det{\\tilde{\\Theta}(z)} \\neq 0, \\forall z \\in \\mathbb{C} \\ s.t. \\ |z|&lt;1\\), with \\(E[\\tilde{\\varepsilon}_t \\tilde{\\varepsilon}_s] = 0\\) but \\(\\tilde{\\varepsilon}_t\\) not independent on \\(\\tilde{\\varepsilon}_s\\). A consequence of Th. 1.1 is that standard methods for the estimation of VAR models do in general fail to identify the correct representation of the process. Indeed, if \\(\\Theta(L)\\) is not invertible, the VAR approximation becomes \\(\\tilde{A}(L) Y_t = \\tilde{\\varepsilon}_t\\), with \\(\\tilde{A}(L) = \\tilde{\\Theta}(L)^{-1}\\Phi(L)\\). Moreover, if we consider the process \\(\\tilde{Y}_t\\) such that \\(\\Phi(L) \\tilde{Y}_t = \\tilde{\\Theta}(L) \\tilde{C} \\eta_t\\), with \\(V(\\tilde{\\varepsilon}_t) = \\tilde{C}\\tilde{C}&#39;\\), the two processes \\(Y_t, \\tilde{Y}_t\\) have the same second-roder dynamic properties, namely the same autocovariance functions. This entails that all the methods that relies on distinguishing different second-order properties (like the Yule-Walker equations) fail to distinguish between \\(\\Theta(L)\\) and \\(\\tilde{\\Theta}(L)\\). However, the impulse response functions associated to shocks in \\(Y_t\\) and \\(\\tilde{Y}_t\\) are different. Example 1.2 (2nd order properties and IRF) Consider the MA(1) processes \\[ y_t = \\sigma \\eta_t - \\theta \\sigma \\eta_{t-1} \\\\ \\tilde{y}_t = \\sigma \\theta \\eta_t - \\sigma \\eta_{t-1} \\] with \\(E[\\eta_t] = 0, E[\\eta_t^2]= 1, \\theta\\sigma&gt;1\\). The two processes have the same autocovariance functions: \\[ \\begin{align} \\text{cov}(y_t, y_{t-h}) = \\text{cov}(\\tilde{y}_t, \\tilde{y}_{t-h}) = \\begin{cases} E[y_t^2] &amp;= E[\\tilde{y}_t^2] &amp;= \\sigma^2(1+\\theta^2) \\quad &amp; \\text{for } h = 0\\\\ E[y_t y_{t-1}] &amp;= E[\\tilde{y}_t \\tilde{y}_{t-1}] &amp;= -\\theta \\sigma^2 \\quad &amp; \\text{for } h = 1 \\\\ 0 &amp;&amp;&amp; \\text{for } h \\geq 2 \\end{cases} \\end{align} \\] However, \\(y_t\\) and \\(\\tilde{y}_t\\) respond differently to a shock in \\(\\eta_t\\): \\[ \\begin{align} \\frac{\\partial y_{t+h}}{\\partial \\eta_t} = \\begin{cases} \\sigma \\quad &amp; \\text{for } h = 0 \\\\ -\\theta \\sigma \\quad &amp; \\text{for } h = 1\\\\ 0 \\quad &amp; \\text{for } h \\geq 2 \\end{cases}; \\qquad \\frac{\\partial \\tilde{y}_{t+h}}{\\partial \\eta_t} = \\begin{cases} \\sigma \\theta \\quad &amp; \\text{for } h = 0 \\\\ -\\sigma \\quad &amp; \\text{for } h = 1\\\\ 0 \\quad &amp; \\text{for } h \\geq 2 \\end{cases} \\end{align} \\] The methods that try to distinguish fundamental and non-fundamental MA coefficients relying on the second-order properties of the process cannot work in this context. For this reason, GMR2019 proposes a new estimation procedure for SSVARMA models that relies on higher-order conditions for identifying the true structural shocks. Of course, the method is suitable for any distribution of the structural shocks with the only exception of the Gaussian distribution, whose higher-order moments are null whatever the mean and variance. "],["gmr-estimation-method.html", "Section 2 GMR estimation method 2.1 Maximum likelihood approach 2.2 Semi-parametric estimation: Two-stage 2SLS-GMM approach", " Section 2 GMR estimation method As we introduced in the previous Section, non-Gaussianity of the shocks is a necessary condition for identifiability. Hp.5 (Non-Gaussianity of the shocks). Each component of \\(\\eta_t\\) has a non-zero \\(r\\)-th cumulant with \\(r&gt;2\\) and at least one finite moment \\(s\\geq r\\). The following Theorem provides one of the main results in GMR2019. Theorem 2.1 (Observational equivalence of SSVARMA processes). Consider the SSVARMA processes \\(Y_t, \\tilde{Y}_t\\) defined by \\[ \\begin{align} \\Phi(L) Y_t &amp;= \\Theta(L) C \\eta_t\\\\ \\Phi(L) \\tilde{Y}_t &amp;= \\tilde{\\Theta}(L) \\tilde{C} \\tilde{\\eta}_t \\end{align} \\] Under Hp. 1-5, \\(Y_t\\) and \\(\\tilde{Y}_t\\) are observationally equivalent if and only if \\(\\Theta(L) = \\tilde{\\Theta}(L)\\), \\(C = \\tilde{C}\\) and the distribution of \\(\\eta_t\\) and \\(\\tilde{\\eta}_t\\) coincide. As a direct consequence of Th. 2.1, if processes characterized by Hp.1-5 differ for the MA part, then they can be distinguished by exploiting higher-order conditions. A last assumption that we make to assure global identifiability (i.e. to identify the coefficients with no ambiguity regarding their sign and order) is the following: Hp.6 (Global identifiability). The components of the first row of \\(C\\) are non-negative and in increasing order. The next two sections present the two methods developed by GMR2019. The first one is a parametric maximum-likelihood approach that requires to assume the distribution of the structural shocks in advance. The second one is a semi-parametric approach consisting in a two-step procedure. Even if it has the advantage of requiring no assumptions on the distribution of the shocks, it is less efficient than the ML procedure when the true distribution is known. Of course, both methods require that the distribution of the shocks is not Gaussian to exploit higher-moments for identification. 2.1 Maximum likelihood approach 2.1.1 MA(1) case Consider the simple MA(1) case first: \\(y_t = \\varepsilon_t + \\theta \\varepsilon_{t-1}\\), where \\(\\varepsilon_t\\) is i.i.d. over time, with a common non-Gaussian p.d.f. \\(g(\\varepsilon; \\gamma)\\), \\(\\gamma\\) being a vector of unknown parameters. We have to derive the log-likelihood of a sample of observed values \\((y_1, y_2, \\dots, y_T)\\), considering the three cases \\(|\\theta|&lt;1, \\ |\\theta| &gt;1, \\ |\\theta| = 1\\). To this end, we have to express the vector of shocks as a function of the observed values and of the parameters we want to estimate (in this case, \\(\\theta\\)). Since the precise expression of the shocks as a function of the past values of \\(y_t\\) would require a perfect knowledge of the infinite past before the shock, while we only observe a finite sample of the past, we need consider the truncated approximation of that expression. Therefore, the method is more precisily a truncated maximum-likelihood approach. Finally, we will substitute this expression into the p.d.f. of the shocks. Since the shocks are independent over time, their joint probability is the product of the probability of each draw. The maximum likelihood estimator will be the value of \\(\\theta\\) that maximizes the value of the likelihood function given the observed sample. \\(|\\theta|&lt;1\\): We can express \\(\\varepsilon_t\\) as a function of \\(\\theta\\) by exploiting standard invertibility: \\[ \\varepsilon_t(\\theta) = (1-\\theta L)^{-1} y_t = \\sum_{h = 0}^\\infty \\theta^h y_{t-h} = \\sum_{h = 0}^{t-1} \\theta^h y_{t-h} + \\theta^t \\underbrace{ \\sum_{h = 0}^{\\infty} \\theta^h y_{-h}}_{ = \\varepsilon_0(\\theta) \\text{ (unobserved)} } \\] The truncated log-likelihood expresses the probability of the observable approximation \\(\\varepsilon_t(\\theta) - \\varepsilon_0(\\theta)\\) as a function of the parameter \\(\\theta\\), for values \\(|\\theta|&lt;1\\): \\[ L_1(\\theta; y_1, \\dots, y_T, \\gamma) = \\log \\left(\\prod_{t=1}^Tg(\\varepsilon_t(\\theta) - \\varepsilon_0(\\theta); \\gamma)\\right) = \\sum_{t = 1}^T \\log g \\left(\\sum_{h = 0}^{t-1} \\theta^h y_{t-h}; \\gamma\\right), \\] where we have exploited the fact that the shocks are independent draws from the same distribution to express the joint probability \\(g_{\\varepsilon_1, \\dots, \\varepsilon_T}\\) as the product of the common marginal distribution \\(g\\). \\(\\theta&gt;1\\): for this case, we have to compute the likelihood that the observed sample came from a non-fundamental representation. As before, we have to express the shocks as a function of the observed values, by exploiting the generalized invertibility of the shocks in the future: \\[ y_t = (1+\\theta L) \\varepsilon_t = \\theta L(1+\\theta^{-1} L^{-1}) \\] from which \\[ \\begin{align} \\varepsilon_t(\\theta) &amp;= \\theta^{-1} L^{-1} (1+\\theta^{-1} L^{-1})^{-1} y_t = \\theta^{-1} L^{-1}(1+ \\theta^{-1} L^{-1} + \\theta^{-2} L^{-2} + \\dots ) y_t = = \\sum_{h = 1}^{\\infty} \\theta^{-h} y_{t+h} = \\\\ &amp; = \\sum_{h = 1}^{T-t} \\theta^{-h} y_{t+h} + \\theta^{-T-t} \\underbrace{ \\sum_{h = 1}^{\\infty} \\theta^{-h} y_{T+h}}_{\\varepsilon_T(\\theta) \\text{ (unobserved)}} \\end{align} \\] The truncated log-likelihood of \\(\\varepsilon_t(\\theta) - \\varepsilon_T(\\theta)\\) for \\(|\\theta|&gt;1\\) is: \\[ L_2(\\theta; y_1, \\dots, y_T, \\gamma) = \\sum_{t = 0}^{T-1} \\log\\left[ \\frac{1}{| \\theta|}g \\left(\\sum_{h = 1}^{T-t} \\theta^{-h} y_{t+h}; \\gamma\\right) \\right] \\] The factor \\(\\frac{1}{|\\theta|}\\) (which can be collected separately from the previous expression as \\(T \\log(1/|\\theta|)\\)) comes from the Jacobian equation that can be derived from the following well known theorem (Mood et al., p.211, Th. 15). Theorem 2.2 (P.d.f. of a transformed random vector) Consider \\[ \\begin{align} y_2 &amp; = f_1(\\varepsilon_1, \\varepsilon_2, \\dots, \\varepsilon_T) = \\varepsilon_{2} + \\theta \\varepsilon_{1}\\\\ y_3 &amp; = f_2(\\varepsilon_1, \\varepsilon_2, \\dots, \\varepsilon_T) = \\varepsilon_{3} + \\theta \\varepsilon_{2}\\\\ &amp; \\vdots \\\\ y_{T} &amp; = f_{T}(\\varepsilon_1, \\varepsilon_2, \\dots, \\varepsilon_T) = \\varepsilon_{T} + \\theta \\varepsilon_{T-1}\\\\ \\end{align} \\] (\\(f_1, \\dots, f_{T-1}\\) are one-to-one transformations of jointly continuous random variables with p.d.f. \\(g_{\\varepsilon_{2}, \\dots, \\varepsilon_{T}}(\\varepsilon_{2}, \\dots, \\varepsilon_{T})\\)). Consider also the inverse transformation \\[ \\begin{align} \\varepsilon_1(\\theta; y_1,\\dots, y_T) &amp;= f_1^{-1}(y_1,\\dots, y_T) = \\theta^{-1}y_2+ \\theta^{-2}y_3 +\\dots+ \\theta^{-(T-1)}y_T \\\\ \\varepsilon_2(\\theta; y_1,\\dots, y_T) &amp;= f_2^{-1}(y_1,\\dots, y_T) = 0 + \\theta^{-1}y_3+ \\theta^{-2}y_4 +\\dots+ \\theta^{-(T-2)}y_{T-1}\\\\ &amp; \\vdots \\\\ \\varepsilon_{T-1}(\\theta; y_1,\\dots, y_T) &amp;= f_{T-1}^{-1}(y_1,\\dots, y_T) = 0+0+\\dots+ 0+\\theta^{-1}y_T \\end{align} \\] Define \\(J\\) as the Jacobian of \\(f^{-1} = (f_1^{-1}, \\dots, f_{T-1}^{-1})\\): \\[ J = \\begin{bmatrix} \\frac{\\partial f_1^{-1}}{\\partial y_1} &amp; \\dots &amp; \\frac{\\partial f_1^{-1}}{\\partial y_{T-1}} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f_{T-1}^{-1}}{\\partial y_1} &amp; \\dots &amp; \\frac{\\partial f_{T-1}^{-1}}{\\partial y_{T-1}} \\end{bmatrix} = \\begin{bmatrix} \\theta^{-1} &amp; \\theta^{-2} &amp; \\dots &amp; \\theta^{-(T-1)} \\\\ 0 &amp; \\theta^{-1} &amp; \\dots &amp; \\theta^{-(T-2)} \\\\ \\vdots &amp;&amp;&amp; \\\\ 0 &amp; 0 &amp; \\dots &amp; \\theta^{-1} \\end{bmatrix} \\] Then the p.d.f. of the transformed vector \\(\\mathbf{y} = f(\\mathbf{\\varepsilon})\\) can be obtained as \\[ g_{y_1, \\dots, y_T} (.)= \\det J \\ g_{\\varepsilon_1, \\dots, \\varepsilon_T} (.) \\] The determinant of \\(J\\) is \\(\\det J = \\frac{1}{\\theta^{T}}\\). The truncated log-likelihood function is given by: \\[ L(\\theta; y_1, \\dots, y_T, \\gamma) = L_1(\\theta; y_1, \\dots, y_T, \\gamma) 1_{|\\theta|&lt;1} + L_2(\\theta; y_1, \\dots, y_T, \\gamma) 1_{|\\theta|\\geq1} \\] The function \\(L\\) is not continuous in \\(\\theta = 1\\). However, the exact likelihood function is continuous and differentiable. Indeed, it is given by: \\[ \\mathcal{L}(\\theta; y_1, \\dots, y_T, \\gamma) = \\log \\left[ \\int g(\\varepsilon_0 | y_1, \\dots, y_T; \\gamma) \\ d\\varepsilon \\right] \\propto \\log \\left[ \\int g(y_1, \\dots, y_T | \\varepsilon_0; \\gamma) g(\\varepsilon_0; \\gamma) \\ d \\varepsilon\\right], \\] which is in general differentiable, as the argument of the integral is a product of differentiable functions. Even though the exact likelihood cannot be directly computed, many properties of the truncated likelihood are asymptotically equivalent to those of the exact likelihood, and proving its properties is therefore useful. Proposition 2.1 (Consistency of the truncated maximum likelihood estimator, Proposition 2 in GMR2019) Let \\(\\Lambda_0\\) be the true vector of parameters of a SSVARMA(p,q) model, and let \\(\\hat{\\Lambda}_T, \\hat{\\Lambda}_T^u\\) denote respectively the truncated and untruncated maximum likelihood estimators of \\(\\Lambda_0\\). Under Hp. 1-6 and under some further regularity conditions, \\[ \\hat{\\Lambda}_T, \\hat{\\Lambda}_T^u \\ \\xrightarrow{a.s.} \\ \\Lambda_0 \\] Moreover, the two estimators are asymptotically normal. 2.1.2 SSVARMA(p,1) case The multivariate case requires some methematical expedients. Let us consider first the SSVARMA(p,1) case: the general SSVAMRA(p,q) estimation will then be a naural extension. The process is: \\[ \\Phi(L) Y_t = \\varepsilon_t + \\Theta \\varepsilon_{t-1} \\] where the errors are linear combinations of the i.i.d. structural shocks \\(\\eta_t\\) with independent components: \\(\\varepsilon_t = C \\eta_t\\), with \\(E[\\eta_{t}] = 0\\), \\(V[\\eta_{t}] = 1\\). The p.d.f. of the errors is \\(g(\\varepsilon_t; C, \\gamma)\\). As a first step, consider the Schur decomposition of matrix \\(\\Theta\\): \\[ \\Theta = A&#39;_{\\Theta} U_{\\Theta} A_{\\Theta}, \\] where \\(A_{\\Theta}\\) is an orthogonal matrix (meaning that \\(A^{-1}_{\\Theta} = A&#39;_{\\Theta}\\)) and \\(U_{\\Theta}\\) is an upper block-triangular matrix, whose diagonal blocks cointains the eigenvalues of \\(\\Theta\\). The \\(1 \\times 1\\) blocks are the real eigenvalues, the \\(2 \\times 2\\) blocks contains the complex eigenvalues \\(\\lambda, \\overline{\\lambda}\\) in the form \\[ U_{k\\Theta} = \\begin{bmatrix} \\text{Re}(\\lambda) &amp; \\text{Im}(\\lambda) \\\\ - \\text{Im}(\\lambda) &amp; \\text{Re}(\\lambda) \\end{bmatrix}, \\] \\(n_k \\in \\{1,2\\}\\) will denote the dimension of the matrix \\(U_{k\\Theta}\\). We can left-multiply \\(\\Phi(L) Y_t = \\varepsilon_t + \\Theta \\varepsilon_{t-1}\\) by \\(A_{\\Theta}&#39;\\) to get a VMA(1) representation of the process. Define \\(W_t = A_{\\Theta}&#39;\\Phi(L) Y_t\\) and \\(\\varepsilon^*_t = A_{\\Theta}&#39; \\varepsilon_t\\) to get \\[ W_t = \\varepsilon_t^* - U_\\Theta \\varepsilon_{t-1}^* \\] Without loss of generality, we assume that the block-diagonal elements \\(U_{k\\Theta}\\) have eigenvalues larger than one for \\(k \\in \\{1, \\dots ,s\\}\\) and eigenvalues smaller than one for \\(k \\in \\{s+1, \\dots , K\\}\\). We can also define \\(\\varepsilon_t^{(1)}, \\varepsilon_t^{(2)}\\) such that \\(\\varepsilon_t = [\\varepsilon_t&#39;^{(1)}, \\varepsilon_t&#39;^{(2)}]&#39;\\), where \\(\\varepsilon_t^{(1)}\\) has length equal to the non-fundamentalness order \\(m = n_1 + \\dots + n_s\\) and \\(\\varepsilon_t^{(2)}\\) has length equal to \\(n-m\\). Similarly we also write \\(W_t = [W_t&#39;^{(1)}, W_t&#39;^{(2)}]&#39;\\). We have: \\[ \\begin{bmatrix} \\varepsilon_t^{(1)} \\\\ \\varepsilon_t^{(2)} \\end{bmatrix} = \\begin{bmatrix} W_t^{(1)} \\\\ W_t^{(2)} \\end{bmatrix} + \\begin{bmatrix} U_\\Theta^{(1)} &amp; U_\\Theta^{(12)} \\\\ 0 &amp; U_\\Theta^{(2)} \\end{bmatrix} \\begin{bmatrix} \\varepsilon_{t-1}^{(1)} \\\\ \\varepsilon_{t-2}^{(2)} \\end{bmatrix} \\] The shocks can now be expressed as a function of \\(W_t\\) (which is in turn a function of the data and the parameters). Moreover, we can distinguish different expression for the errors associated with eigenvalues smaller or bigger than one. In particular, in analogy with the MA(1) case, we can express \\(\\varepsilon_t^{(2)}\\) as a linear combination of the present and past values of the data, and \\(\\varepsilon_t^{(1)}\\) as a linear combination of the present and future values of the data: \\[ \\begin{align} \\varepsilon_t^{*(2)} &amp; = W_t^{(2)} + U_\\Theta^{(2)}W_{t-1}^{(2)} + [U_\\Theta^{(2)}]^2 W_{t-2}^{(2)} + \\dots + [U_\\Theta^{(2)}]^{t-1} W_{1}^{(2)} + \\underbrace{[U_\\Theta^{(2)}]^t \\varepsilon^{*(2)}_0}_{\\text{unobserved}}\\\\ \\varepsilon_t^{*(1)} &amp; = \\underbrace{[U_\\Theta^{(1)}]^{-(T-t)} \\varepsilon_T^{*(1)}}_{\\text{unobserved}} - [U_\\Theta^{(1)}]^{-1} W_{t+1}^{*(1)} - [U_\\Theta^{(1)}]^{-2} W_{t+2}^{*(1)} - \\dots - [U_\\Theta^{(1)}]^{-(T-t)} W_{T}^{*(1)} + \\\\ &amp; - \\underbrace{[U_{\\Theta}^{(1)}]^{-1}U_{\\Theta}^{(12)} \\varepsilon_t^{*(2)}}_{\\text{truncated}} - \\dots - \\underbrace{[U_{\\Theta}^{(1)}]^{-(T-t)}U_{\\Theta}^{(12)} \\varepsilon_{T-1}^{*(2)}}_{\\text{truncated}} \\end{align} \\] Keeping in mind that \\(\\varepsilon_t = A_{\\Theta} \\varepsilon_t^{*}\\), we can now write the truncated log-likelihood function. Let \\(\\Lambda = \\{\\Phi_1, \\dots, \\Phi_p, \\Theta, C, \\gamma\\}\\) be the vector of parameters to be estimated and \\(\\lambda_i(\\Theta)\\) the \\(i\\)-th eigenvalue of \\(\\Theta\\). The truncated versions of \\(\\varepsilon_t^{*(1)}, \\varepsilon_t^{*(2)}\\), which depend only on the model parameters and on observed values, are: \\[ \\begin{align} \\varepsilon_t^{*(2)}|_{truncated} &amp;= W_t^{(2)} + U_\\Theta^{(2)}W_{t-1}^{(2)} + [U_\\Theta^{(2)}]^2 W_{t-2}^{(2)} + \\dots + [U_\\Theta^{(2)}]^{t-1} W_{1}^{(2)} \\\\ \\varepsilon_t^{*(1)}|_{truncated} &amp;= [U_\\Theta^{(1)}]^{-1} W_{t+1}^{*(1)} - [U_\\Theta^{(1)}]^{-2} W_{t+2}^{*(1)} - \\dots - [U_\\Theta^{(1)}]^{-(T-t)} W_{T}^{*(1)} + \\\\ &amp; - [U_{\\Theta}^{(1)}]^{-1}U_{\\Theta}^{(12)} (W_t^{(2)} + U_\\Theta^{(2)}W_{t-1}^{(2)} + \\dots + [U_\\Theta^{(2)}]^{t-1} W_{1}^{(2)})- \\dots + \\\\ &amp; - [U_{\\Theta}^{(1)}]^{-(T-t)}U_{\\Theta}^{(12)} (W_{T-1}^{(2)} + U_\\Theta^{(2)}W_{T-2}^{(2)} + \\dots + [U_\\Theta^{(2)}]^{T-2} W_{1}^{(2)} ) \\end{align} \\] By expressing \\(\\varepsilon^*_t\\) as a function of the observed data and the model parameters, we can set up the Jacobian equation to get \\[ L_{truncated}(\\Lambda) = -T \\sum_{i = 1}^{n} \\log |\\lambda_i(\\Theta)| 1_{|\\lambda_i(\\Theta)|&gt;1} + \\sum_{t = 1}^T \\log g\\left[A_{\\Theta} \\begin{pmatrix} \\varepsilon_t^{*(1)}|_{truncated} \\\\ \\varepsilon_t^{*(2)}|_{truncated} \\end{pmatrix} ; C, \\gamma \\right] \\] Again, Prop. 2.1 applies and the truncated ML estimator \\(\\hat{\\Lambda} = \\arg\\max_\\Lambda L_{truncated}(\\Lambda)\\) is a consistent and asymptotically normal estimator of the true \\(\\Lambda\\). 2.1.3 SVARMA(p,q) case The general case where \\(\\Theta(L)\\) is of order \\(q&gt;1\\) can be reduced to the previous one. Define \\[ \\begin{align} \\tilde{\\varepsilon}_t &amp;= [\\varepsilon_t&#39;, \\dots, \\varepsilon_{t-q+1}&#39;]&#39; \\\\ \\tilde{Y}_t &amp;= [Y_t&#39;, 0_{1 \\times(n-1)q}]&#39; \\\\ \\tilde{\\Phi_k} &amp;= \\mathbf{uu}&#39; \\otimes \\Phi_k, \\quad \\text{where} \\ \\ \\mathbf{u} = [1,0,\\dots,0]&#39;\\\\ \\\\ \\tilde{\\Theta} &amp;= \\begin{bmatrix} \\Theta_1 &amp; \\Theta_2 &amp; \\dots &amp; \\Theta_{q-1} &amp; \\Theta_q \\\\ I &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; I &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; I &amp; 0 \\\\ \\end{bmatrix} \\end{align} \\] where the eigenvalues of \\(\\tilde{\\Theta}\\) are the reciprocal of the roots of \\(\\det \\Theta(z)\\). Then, the process can be rewritten as \\[ \\tilde{\\Phi}(L) \\tilde{Y}_t = \\tilde{\\varepsilon}_t - \\tilde{\\Theta}\\tilde{\\varepsilon}_{t-1} \\] which reduces to the previous case (even though the process is not a VARMA(p,1), because \\(\\tilde{\\varepsilon}_t\\) is not a white noise). It is useful to stress the role that non-Gaussianity plays in this method. Indeed, it is not immediately clear why we should not choose the p.d.f. of the Normal distribution as our \\(g(.)\\). The problem is that with a Gaussian p.d.f., the limiting likelihood function (if the infinite time series were observed) would assume the same value both in the true value of the paramater and in its fundamenral representation. In the practical case of a finite sample, the likelihood would be maximum in one point, but it would be impossible to say whether the estimate approximates the true value of the parameters or their fundamental representation. The following example clarifies this case. Example 2.1 (The role of non-Gaussianity) Consider the MA(1) process \\(y_t = \\varepsilon_t - \\theta \\varepsilon_{t-1}\\), with \\(\\varepsilon_t \\sim N(0, \\sigma^2)\\), i.i.d. and mutually independent. Consider the value of the likelihood function if the whole time series \\(\\{y_t\\}_{t=-\\infty}^{+\\infty}\\) were observed: \\[ L_\\infty(\\theta; y_1, \\dots, y_T, \\gamma) = 1_{|\\theta|&lt;1} \\tilde{L}_1((\\theta; y_1, \\dots, y_T, \\gamma) + 1_{|\\theta|\\geq1} \\tilde{L}_2((\\theta; y_1, \\dots, y_T, \\gamma) \\] where, in the Gaussian case: \\[ \\begin{align} \\tilde{L}_1(\\theta; y_1, \\dots, y_T, \\gamma) &amp;= E_0 \\log g \\left(\\sum_{h = 0}^{+\\infty} \\theta^h y_{t-h}; \\gamma\\right) =\\\\ &amp;=E_0\\log \\left[ \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left( -\\frac{1}{2}\\frac{(\\sum_{h = 0}^{+\\infty} \\theta^h y_{t-h})^2}{\\sigma^2} \\right) \\right] = \\\\ &amp; = -\\frac{1}{2} \\log(2\\pi) -\\frac{1}{2} \\log \\sigma^2 - \\frac{1}{2\\sigma^2} E_0 \\left[\\left(\\sum_{h = 0}^{+\\infty} \\theta^h y_{t-h}\\right)^2\\right]\\\\ \\tilde{L}_2(\\theta; y_1, \\dots, y_T, \\gamma) &amp;= E_0 \\log \\frac{1}{|\\theta|} g \\left(-\\sum_{h = 0}^{+\\infty} \\frac{1}{\\theta^{h+1}} y_{t+h+1}; \\gamma\\right) = \\\\ &amp; = -\\frac{1}{2} \\log(2 \\pi) - \\frac{1}{2} \\log(\\theta^2\\sigma^2) - \\frac{1}{2\\theta^2 \\sigma^2} E_0 \\left[\\left(\\sum_{h = 0}^{+\\infty} \\frac{1}{\\theta^h} y_{t+h+1}\\right)^2\\right] \\end{align} \\] The asymptotic log-likelihood \\(L_\\infty\\) reaches its minimum at both \\((\\theta, \\sigma^2)\\) and \\((\\frac{1}{\\theta}, \\theta^2 \\sigma^2)\\). Therefore, \\(\\theta\\) and \\(\\sigma\\) are not identified, and the finite sample ML estimation cannot distinguish between the fundamental and non-fundamental representation of the same process. 2.2 Semi-parametric estimation: Two-stage 2SLS-GMM approach The main drawback of maximum likelihood estimation is that we need to choose a priori a probability density function for the structural shocks, which is in general unknown. We could think of some methods to choose which distribution of the shocks best fits the data. For instance, we could compare the likelihood of the same model with different p.d.f.s and choose the functional form which gives the likelihood function with highest maximum. However, for the \\(q = 1\\) case, GMR2019 propose a different approach, namely a semi-parametric estimation method which makes no prior assumption on the distribution of the shocks. Consider a SVARMA(p,1) process: \\[ \\begin{equation} Y_t = \\Phi_1 Y_{t-1} + \\dots + \\Phi_p Y_{t-p} + C\\eta_t + C \\Theta \\eta_{t-1} \\tag{2.1} \\end{equation} \\] The procedure consists of two steps. First, we use two-stage least squares (2SLS) to estimate \\(\\Phi_1, \\dots, \\Phi_p\\) and \\(Z_t = C\\eta_t + C \\Theta \\eta_{t-1}\\) as \\(\\hat{Z}_t = Y_t - (\\hat\\Phi_1 Y_{t-1} + \\dots + \\hat\\Phi_p Y_{t-p})\\). Indeed, estimating directly Eq. (2.1) would lead to biased estimates, because the error terms are correlated with \\(Y_{t-1}\\): \\(E[Y_{t-1} Z_t] = E[(\\Phi_1 Y_{t-2} + \\dots + \\Phi_{p} Y_{t-p-1} + C\\eta_{t-1} + C \\Theta \\eta_{t-2})( C\\eta_t + C \\Theta \\eta_{t-1}) = C^2\\Theta E[\\eta_{t-1}^2] \\neq 0\\). On the contrary, we can use \\(Y_{t-2}, \\dots, Y_{t-1-k}, k \\geq p\\) as instruments, whose exogeneity is guaranteed by \\(E[ C\\eta_t + C \\Theta \\eta_{t-1}| Y_{t-2}, \\dots, Y_{t-1-k}] = 0\\), to obtain consistent estimates \\(\\Phi_1\\) and, in turn, of \\(\\hat{Z}_t\\). Note that the condition \\(k\\geq p\\) assures that the first-stage regression of the TSLS estimator has at least one regressor which is not included in the main equation, thus avoiding multicollinearity. The second step exploits moment restrictions on the structural shocks to estimate the mixing matrix \\(C\\) and the matrix of the MA coefficients \\(\\Theta\\) from \\(\\hat{Z}_t\\), in a pure MA framework (note that, as pointed out by GMR2019 themselves, if \\(\\Theta_1 = 0\\) then \\(C\\) can be directly be estimated via ICA). To this aim, the assumption of non-Gaussianity is crucial, since the moment restrictions regard moments higher than the second. To obtain the moment restrictions, we consider the pairwise log-Laplace transform of \\((Z_t, Z_{t-1})\\), i.e. their joint cumulant generating function (c.g.f.). This would allow to impose the conditions on the cumulants, that uniquely identify the distribution of the shocks. For any \\(u,v \\in \\mathbb{R}^n\\), the c.g.f. is \\[ \\begin{align} \\log E[\\exp(u&#39;Z_t + v&#39;Z_{t-1})] &amp;= \\log E[\\exp\\left(u&#39;(C\\eta_t + (\\Theta C) \\eta_{t-1}) + v&#39;(C \\eta_{t-1} + (\\Theta C) \\eta_{t-2}) \\right)] = \\\\ &amp;= \\log E[\\exp(u&#39;(C\\eta_t + (\\Theta C) \\eta_{t-1}) \\exp(v&#39;(C\\eta_{t-1} + (\\Theta C) \\eta_{t-2})] = \\\\ &amp;= \\log \\left( E[\\exp(u&#39;C \\eta_t)] E[\\exp(u&#39;(\\Theta C) \\eta_{t-1})] E[\\exp(v&#39;C \\eta_{t-1})] E[\\exp(v&#39;(\\Theta C) \\eta_{t-2})] \\right) \\tag{2.2} \\end{align} \\] where the last equality comes from the fact that if two generic random variables \\(X, Y\\) are independent (like \\(\\eta_t, \\eta_{t-1}, \\eta_{t-2}\\)), then \\(E[e^{t(X+Y)}] = E[e^{tX}]E[e^{tY}]\\). The c.g.f. in Eq. (2.2) can be expanded as a McLaurin power series of the form \\[ K(w) = \\sum_{n= 1}^\\infty \\kappa_n\\frac{w^n}{n!} \\] where \\(\\kappa_n\\) is the \\(n\\)-th cumulant of the distribution. To apply the moment conditions, we expand the series up to the fourth degree. Since \\(E[\\eta_j] = 0, E[\\eta^2_j] = 1\\), we have \\[ K_{\\eta_j}(w) = \\frac{w^2}{2} + \\frac{w^3}{6}\\kappa_{3j} + \\frac{w^4}{24}\\kappa_{4j} \\] By further algebraic manipulations, we can get the four moment conditions that hold for any couple \\((u,v)\\). \\[ \\begin{align} E[(u&#39;Z_t + v&#39; Z_{t-1})^2] &amp;= \\sum_{j= 1}^n [(u&#39;C_j)^2 + (u&#39;(\\Theta C)_j + v&#39;C_j)^2 + (v&#39;(\\Theta C)_j)^2]\\\\ E[(u&#39;Z_t + v&#39; Z_{t-1})^3] &amp;= \\sum_{j= 1}^n \\kappa_{3j}[(u&#39;C_j)^3 + (u&#39;(\\Theta C)_j + v&#39;C_j)^3 + (v&#39;(\\Theta C)_j)^3] \\\\ E[(u&#39;Z_t + v&#39; Z_{t-1})^4] &amp;= \\sum_{j= 1}^n \\kappa_{4j}[(u&#39;C_j)^4 + (u&#39;(\\Theta C)_j + v&#39;C_j)^4 + (v&#39;(\\Theta C)_j)^4] + \\\\ &amp; + 3\\left(\\sum_{j= 1}^n [(u&#39;C_j)^2 + (u&#39;(\\Theta C)_j + v&#39;C_j)^2 + (v&#39;(\\Theta C)_j)^2]\\right)^2 \\tag{2.3} \\end{align} \\] Therefore, the set of moment restrictions is of the form \\[ E[h(Z_t, Z_{t-1}); \\beta] = 0, \\quad \\text{with } \\beta = [\\text{vec}C&#39;, \\text{vec}(\\Theta C)&#39;, \\kappa_{31}, \\dots, \\kappa_{3n}, \\kappa_{41}, \\dots, \\kappa_{4n}] \\] where \\(h(.)\\) is defined as in Eqs. (2.3). Since the number of parameters to be estimated in \\(\\beta\\) is \\(2n^2+2n\\), the order condition is \\(r\\geq 2n^2+2n\\). Therefore, by exploiting the analogy principle between the population and the sample moments, we can obtain the estimator for \\(\\beta\\) as \\[ \\hat\\beta = \\arg\\min_\\beta \\frac{1}{T}\\sum_{t=1}^Th(\\hat{Z}_t, \\hat{Z}_{t-1}; \\beta) \\] The following example clarifies the procedure in the MA(1) case. Example 2.2 (Moment method in the MA(1) case) In the case of a MA(1), \\(\\theta\\) can be easily identified if the distribution of the shocks is skewed. Indeed: \\[ \\begin{align} E[y_t y_{t-1}^2] &amp;= E[(\\varepsilon_{t}-\\theta \\varepsilon_{t-1})(\\varepsilon_{t-1}-\\theta \\varepsilon_{t-2})^2] = -\\theta E[\\varepsilon_{t}^3] \\\\ E[y_t^2 y_{t-1}] &amp;= E[(\\varepsilon_{t}-\\theta \\varepsilon_{t-1})^2(\\varepsilon_{t-1}-\\theta \\varepsilon_{t-2})] = \\theta^2 E[\\varepsilon_{t}^3] \\end{align} \\] Therefore, whenever \\(E[\\varepsilon_{t}^3] \\neq 0\\), the parameter is identified as \\[ \\theta = - \\frac{E[y_t^2 y_{t-1}]}{E[y_t y_{t-1}^2]} \\] and can be consistently estimated as \\(\\hat\\theta = - \\frac{\\sum_{t=1}^T y_t y_t^2}{\\sum_{t=1}^T y_t^2 y_t}\\). "],["svarma11-example.html", "Section 3 SVARMA(1,1) example 3.1 Simulation 3.2 Estimation 3.3 Results", " Section 3 SVARMA(1,1) example We now follow the procedure from GMR2019 to simulate a (stationary) structural VARMA process. We will then use both the maximum-likelihood and the two-step 2SLS-GMM procedure proposed by the authors to estimate the model and compare the results with the true values. The process we simulate is the following bivariate SVARMA(1,1): \\[ y_t = \\Phi_1 y_{t-1} + C\\eta_t + \\Theta C\\eta_{t-1}, \\] where \\(C\\) is the mixing matrix and \\(\\eta_t\\) are the independent and non-Gaussian structural shocks of the process. The extensive form of the system is the following: \\[ \\begin{cases} y_{1t} = \\phi_{11} y_{1t-1} + \\phi_{12} y_{2t-1} + c_{11} \\eta_{1t} + c_{12} \\eta_{2t} + (\\theta_{11} c_{11} + \\theta_{12} c_{21}) \\eta_{1t-1} + (\\theta_{11} c_{12} + \\theta_{12} c_{22}) \\eta_{2t-1} \\\\ y_{2t} = \\phi_{21} y_{1t-1} + \\phi_{22} y_{2t-1} + c_{21} \\eta_{1t} + c_{22} \\eta_{2t} + (\\theta_{21} c_{11} + \\theta_{22} c_{21}) \\eta_{1t-1} + (\\theta_{21} c_{12} + \\theta_{22} c_{22}) \\eta_{2t-1} \\end{cases} \\] We can see that the moving average structure of the system greatly increase the complexity of the model with respect to a standard SVAR(1) process. In particular, the system reacts not only to contemporaneous shocks but also to past shocks in both variables. The entity of the reaction is described by \\(\\Theta\\), which must be identified together with the mixing matrix \\(C\\). In a nutshell, the problem of non-fundamentalness mainly consists on the inadequacy of standard estimation techniques when a coefficient \\(\\theta_{ij}\\) is greater than one in modulus. This can actually be e frequent case in practice, since it means that some shocks may have a lagged impact and unfold their effects after one or more periods. Below, I adapt the code provided by GMR2019 to show a minimal practical example of their estimation technique using simulated data. 3.1 Simulation To illustrate the method and evaluate its performances, we simulate different processes characterized by different distributions of the structural shocks. In particular, we will draw the shocks from three different combinations: two Gaussian mixtures; one Gaussian mixture and one Student t; two Student t. We will show that the method is imprecise when the distribution of one of the shocks is a Student t with many degrees of freedom, because this is similar to the Gaussian case and identification based on higher order moments fails. On the contrary, a precise estimation is achieved when the shocks are distributed as Gaussian mixtures, since the moments higher than the second are distant in the (true) non-fundamental process and its fundamental representation. For the purposes of the simulation study, we have to generate shocks whose distribution has skewness and kurtosis that are set in advance. To this end, we have to choose the parameters of the distribution which are consistent with the chosen skewness and kurtosis. This passage requires a bit of algebraic manipulations in the case of the mixture Gaussian distribution. Consider the following p.d.f. of a Gaussian mixture: \\[ g(\\mu_1, \\mu_2, \\sigma_1, \\sigma_2, p) = p \\ N(\\mu_1, \\sigma_1) + (1-p) \\ \\ N(\\mu_2, \\sigma_2) \\] where \\(N(\\mu_i, \\sigma_i) = \\frac{1}{\\sigma_i \\sqrt{2\\pi}} \\exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu_i}{\\sigma_i}\\right)^2\\right]\\) denotes the p.d.f. of the Normal distribution. We want to fix the third and fourth cumulants of the distribution and compute the parameters \\(\\mu_1, \\mu_2, \\sigma_1, \\sigma_2\\) accordingly. Let \\(X \\sim g(\\mu_1, \\mu_2, \\sigma_1, \\sigma_2, p)\\), with \\(X_1 \\sim N(\\mu_1, \\sigma_1), \\ X_2 \\sim N(\\mu_2, \\sigma_2)\\). We impose: \\[ \\begin{align} E[X] &amp;= 0 \\\\ E[X^2] &amp;= 1 \\\\ E[X^3] &amp;= \\kappa_3 \\\\ E[X^4] &amp;= \\kappa_4 \\end{align} \\] We also know that \\[ \\begin{align} E[X_i] &amp;= \\mu_i \\\\ E[X_i^2] &amp;= \\sigma_i^2+\\mu_i^2 \\\\ E[X_i^3] &amp;= \\mu_i^3 + 3\\mu_i\\sigma_i^2 \\\\ E[X_i^4] &amp;= \\mu_i^4 + 6\\mu_i^2\\sigma_i^2 + 3\\sigma_i^4 \\end{align} \\] and \\[ E[h(X)] = p E[h(X_1)] + (1-p)E[h(X_2)] \\] From the previous expressions we get: \\[ \\begin{align} p \\mu_1 + (1-p) \\mu_2 &amp;= 0 \\implies \\mu_2 = -\\frac{p}{1-p} \\mu_1 \\\\ p (\\sigma_1^2 + \\mu_1^2) + (1-p) (\\sigma_2^2 + \\mu_2^2) &amp;= 1 \\implies \\sigma_2^2 = \\frac{1}{1-p}\\left(1 - p\\sigma_1^2 - \\frac{p}{1-p}\\mu_1^2\\right) \\\\ p(\\mu_1^3 + 3\\mu_1\\sigma_1^2) + (1-p)(\\mu_2^3 + 3\\mu_2\\sigma_2^2) &amp;= \\kappa_3\\implies \\sigma_1^2 = \\frac{1-p}{3p \\mu_1} \\left[\\kappa_3 - \\frac{p(1+p)}{(1-p)^2} \\mu_1^3 + \\frac{3p\\mu_1}{1-p}\\right] \\end{align} \\tag{3.1} \\] We are left to find a parameter \\(\\mu_1\\) consistent with \\(\\kappa_3, \\kappa_4\\). To this end, in the simulation we proceed as follows: Initialize e vector of \\(\\tilde{\\mu}_1\\) in regular intervals of (-30, 30) For each value in \\(\\tilde{\\mu}_1\\), compute the corresponding \\(\\tilde{\\mu}_2, \\tilde{\\sigma}_1, \\tilde{\\sigma}_2\\) using the formulas in Eq. (3.1). For each of the resulting Gaussian mixture \\(g(\\tilde{\\mu}_1, \\tilde{\\mu}_2, \\tilde{\\sigma}_2, \\tilde{\\sigma}_2)\\), compute the kurtosis \\[ \\tilde{\\kappa}_4(\\tilde{\\mu}_1) = p(\\tilde\\mu_i^4 + 6\\tilde\\mu_i^2\\tilde\\sigma_i^2 + 3\\tilde\\sigma_i^4) + (1-p)(\\tilde\\mu_i^4 + 6\\tilde\\mu_i^2\\tilde\\sigma_i^2 + 3\\tilde\\sigma_i^4) \\] Choose the \\(\\mu_1\\) that minimizes the distance from the desired \\(\\kappa_4\\): \\[ \\mu_1 = \\arg \\min_{\\tilde{\\mu_1}} [\\tilde{\\kappa}_4(\\tilde{\\mu}_1) - \\kappa_4]^2 \\] Moreover, in their code, GMR2019 introduce a different parametrization of the Gaussian mixture, that is in one-to-one relation with the original one but is computationally more stable for their algorithm. It uses three parameters for each distribution that are trigonometric transformations of the original parameters. ## Run relevant packages library(optimx) # Optimization for maximum likelihood library(vars) ## Loading required package: MASS ## Loading required package: strucchange ## Loading required package: zoo ## ## Attaching package: &#39;zoo&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## as.Date, as.Date.numeric ## Loading required package: sandwich ## Loading required package: urca ## Loading required package: lmtest library(doParallel) # Parallel computing for long executions ## Loading required package: foreach ## Loading required package: iterators ## Loading required package: parallel library(compiler) library(stringr) ## ## Attaching package: &#39;stringr&#39; ## The following object is masked from &#39;package:strucchange&#39;: ## ## boundary library(Matrix) library(tidyverse) ## -- Attaching packages --------------------------------------- tidyverse 1.3.0 -- ## v ggplot2 3.3.2 v purrr 0.3.4 ## v tibble 3.0.3 v dplyr 1.0.3 ## v tidyr 1.1.2 v forcats 0.4.0 ## v readr 1.4.0 ## -- Conflicts ------------------------------------------ tidyverse_conflicts() -- ## x purrr::accumulate() masks foreach::accumulate() ## x stringr::boundary() masks strucchange::boundary() ## x tidyr::expand() masks Matrix::expand() ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ## x tidyr::pack() masks Matrix::pack() ## x dplyr::select() masks MASS::select() ## x tidyr::unpack() masks Matrix::unpack() ## x purrr::when() masks foreach::when() library(ggplot2) ## Set working directory setwd(&quot;C:/Users/Alessandro Ciancetta/Documents/Universita/UniPi/V/TesiEconomics/Papers SSVARMA/Gourieroux2019-Supplementary_2021/Rcodes_2021&quot;) ## Load the code by GMR2019 source(&quot;various_prog/set.of.procedures.R&quot;) As a first step, we define the length of the sample an its dimensionality. T &lt;- 300 # Sample size n &lt;- 2 # Number of variables We now set the model parameters and the initial values of the simulation. ## Model parameters p &lt;- 1 # number of lags Phi &lt;- array(0,c(n,n,p)) # AR coefficients Phi[1,1,1] &lt;- .8 Phi[1,2,1] &lt;- .3 Phi[2,1,1] &lt;- -.3 Phi[2,2,1] &lt;- .5 q &lt;- 1 # MA order Theta &lt;- array(NaN,c(n,n,q)) # MA coefficients (non-fundamental) Theta[,,1] &lt;- diag(c(-.5,-2)) Theta[1,2,1] &lt;- 0 Theta[2,1,1] &lt;- 1 Mu &lt;- rep(0,n) # intercept (null) C &lt;- diag(n) # mixing matrix C[1,1] &lt;- 0 C[2,1] &lt;- 1 C[1,2] &lt;- 1 C[2,2] &lt;- 0.5 ## Initial values for the simulation Y0 &lt;- rep(0,n*p) eta0 &lt;- rep(0,q * n) In the next three sections, we set up three different combinations for the distribution of the shocks. 3.1.1 Two Gaussian mixtures We begin by specifying the model for the case where both the shocks are distributed as Gaussian mixtures. ## Shock 1 distribution p.1 &lt;- .1 # mixing probability kappa.3_1 &lt;- 2 # third cumulant kappa.4_1 &lt;- 6 # fourth cumulant ## Change in parameters used for GMM estimation kappa.3.modif &lt;- c(kappa.3_1) kappa.4.modif &lt;- c(2 + kappa.4_1 - kappa.3_1^2) ## Baseline and alternative parametrization of the Gaussian (for computational convenience) param.mixture.1 &lt;- get.Gaussian.mixture.from.kappa(kappa.3_1,kappa.4_1,p.1) ## [1] &quot;Variance of distri.:1&quot; ## [1] &quot;kappa_3 of distri.:2&quot; ## [1] &quot;kappa_4 of distri.:5.9998955175277&quot; param.distri.1 &lt;- make.theta1.2.3(p.1,param.mixture.1$mu.1,param.mixture.1$sigma.1) ## Shock 2 distribution p.2 &lt;- .5 kappa.3_2 &lt;- 1.1 kappa.4_2 &lt;- 1.5 ## Change in parameters used for GMM estimation kappa.3.modif &lt;- c(kappa.3.modif,kappa.3_2) kappa.4.modif &lt;- c(kappa.4.modif,2 + kappa.4_2 - kappa.3_2^2) ## Baseline and alternative parametrization of the Gaussian (for computational convenience) param.mixture.2 &lt;- get.Gaussian.mixture.from.kappa(kappa.3_2,kappa.4_2,p.2) ## [1] &quot;Variance of distri.:1&quot; ## [1] &quot;kappa_3 of distri.:1.1&quot; ## [1] &quot;kappa_4 of distri.:1.5003007769495&quot; param.distri.2 &lt;- make.theta1.2.3(p.2,param.mixture.2$mu.1,param.mixture.2$sigma.1) ## Summarize the distribution of the shocks in a unique object distri_mixt2 &lt;- list(type=c(&quot;mixt.gaussian&quot;,&quot;mixt.gaussian&quot;), mu=c(param.mixture.1$mu.1,param.mixture.2$mu.1), sigma=c(param.mixture.1$sigma.1,param.mixture.2$sigma.1), p=c(p.1, p.2), df=c(NaN,NaN)) ## Create an object Model Model_mixt2 &lt;- list( Mu = Mu, Phi = Phi, Theta = Theta, C = C, distri = distri_mixt2 ) The function simul.VARMA simulates the SVARMA process using the given parameters and initial values. The shocks are generated according to the distributions specified in the object Model_mixt2$distri. res_mixt2 &lt;- simul.VARMA(Model_mixt2, nb.sim = T+1, Y0,eta0) Y_mixt2 &lt;- t(res_mixt2$Y)[,1:n] data_sim &lt;- tibble(date = 1:nrow(Y_mixt2), Y1 = Y_mixt2[,1], Y2 = Y_mixt2[,2]) %&gt;% pivot_longer(-date, names_to = &quot;factor&quot;, values_to = &quot;value&quot;) %&gt;% mutate(factor = fct_relevel(factor, names(Y_mixt2))) data_sim %&gt;% ggplot(aes(x = date, y = value)) + geom_line() + facet_wrap(vars(factor), nrow = ncol(Y_mixt2), dir = &quot;v&quot;, scales = &quot;free&quot;) + labs(title = &quot;Simulated series (two Gaussian mixtures)&quot;, y = &quot;&quot;, x= &quot;&quot;) We also set the initial values for the estimation procedures of the next sections. ## GMM estimation ## Specify the moments u &lt;- rbind(c(2,0),c(0,2),c(1,0),c(2,0),c(1,0),c(2,0),c(0,1),c(0,2),c(0,1),c(0,2)) v &lt;- rbind(c(0,0),c(0,0),c(2,0),c(1,0),c(0,2),c(0,1),c(0,2),c(0,1),c(2,0),c(1,0)) ## Specify the initial values as the true values + randomness param.ini.0_mixt2 &lt;- c(Model_mixt2$C,Model_mixt2$Theta,kappa.3.modif,kappa.4.modif) # add randomness: param.ini.GMM_mixt2 &lt;- param.ini.0_mixt2 * (1 + .2 * rnorm(length(param.ini.0_mixt2))) ## MLE Estimation ## Set initial values (true values+randomness), ## including the parameters of the distribution of the shocks. param.ini.0_mixt2 &lt;- c(Model_mixt2$Phi, Model_mixt2$Theta, Model_mixt2$C, param.distri.1$theta1,param.distri.2$theta1, param.distri.1$theta2,param.distri.2$theta2, param.distri.1$theta3,param.distri.2$theta3) # add randomness param.ini.MLE_mixt2 &lt;- param.ini.0_mixt2 * (1 + .2 * rnorm(length(param.ini.0_mixt2))) param.ini.MLE_mixt2[abs(param.ini.MLE_mixt2)&gt;10^8] &lt;- 0.01 3.1.2 Gaussian mixture and t-student ## Summarize the distribution of the shocks in a unique object distri_mixt_t &lt;- list(type=c(&quot;mixt.gaussian&quot;,&quot;student&quot;), mu=c(param.mixture.1$mu.1,NaN), sigma=c(param.mixture.1$sigma.1,NaN), p=c(p.1, NaN), df=c(NaN,6)) ## Collect the parameters in the object Model Model_mixt_t &lt;- list( Mu = Mu, Phi = Phi, Theta = Theta, C = C, distri = distri_mixt_t ) ## Simulate the process res_mixt_t &lt;- simul.VARMA(Model_mixt_t, nb.sim = T+1, Y0,eta0) Y_mixt_t &lt;- t(res_mixt_t$Y)[,1:n] data_sim &lt;- tibble(date = 1:nrow(Y_mixt_t), Y1 = Y_mixt_t[,1], Y2 = Y_mixt_t[,2]) %&gt;% pivot_longer(-date, names_to = &quot;factor&quot;, values_to = &quot;value&quot;) %&gt;% mutate(factor = fct_relevel(factor, names(Y_mixt_t))) data_sim %&gt;% ggplot(aes(x = date, y = value)) + geom_line() + facet_wrap(vars(factor), nrow = ncol(Y_mixt_t), dir = &quot;v&quot;, scales = &quot;free&quot;) + labs(title = &quot;Simulated series (Gaussian mixture and Student t)&quot;, y = &quot;&quot;, x= &quot;&quot;) ## GMM estimation ## Specify the moments u &lt;- rbind(c(2,0),c(0,2),c(1,0),c(2,0),c(1,0),c(2,0),c(0,1),c(0,2),c(0,1),c(0,2)) v &lt;- rbind(c(0,0),c(0,0),c(2,0),c(1,0),c(0,2),c(0,1),c(0,2),c(0,1),c(2,0),c(1,0)) ## Specify the initial values as the true values + randomness param.ini.0_mixt_t &lt;- c(Model_mixt_t$C,Model_mixt_t$Theta,kappa.3.modif,kappa.4.modif) # add randomness: param.ini.GMM_mixt_t &lt;- param.ini.0_mixt_t * (1 + .2 * rnorm(length(param.ini.0_mixt_t))) ## MLE Estimation ## Set initial values (true values+randomness), ## including the parameters of the distribution of the shocks. param.ini.0_mixt_t &lt;- c(Model_mixt_t$Phi, Model_mixt_t$Theta, Model_mixt_t$C, param.distri.1$theta1,param.distri.2$theta1, param.distri.1$theta2,param.distri.2$theta2, param.distri.1$theta3,param.distri.2$theta3) # add randomness param.ini.MLE_mixt_t &lt;- param.ini.0_mixt_t * (1 + .2 * rnorm(length(param.ini.0_mixt_t))) param.ini.MLE_mixt_t[abs(param.ini.MLE_mixt_t)&gt;10^8] &lt;- 0.01 3.1.3 Two t-student ## Two t-student distributions distri_t2 &lt;- list(type=c(&quot;student&quot;,&quot;student&quot;), mu=c(NaN,NaN), sigma=c(NaN,NaN), p=c(NaN, NaN), df=c(6,6)) ## Collect the parameters in the object Model Model_t2 &lt;- list( Mu = Mu, Phi = Phi, Theta = Theta, C = C, distri = distri_t2 ) ## Simulate the process res_t2 &lt;- simul.VARMA(Model_t2, nb.sim = T+1, Y0,eta0) Y_t2 &lt;- t(res_t2$Y)[,1:n] data_sim &lt;- tibble(date = 1:nrow(Y_t2), Y1 = Y_t2[,1], Y2 = Y_t2[,2]) %&gt;% pivot_longer(-date, names_to = &quot;factor&quot;, values_to = &quot;value&quot;) %&gt;% mutate(factor = fct_relevel(factor, names(Y_t2))) data_sim %&gt;% ggplot(aes(x = date, y = value)) + geom_line() + facet_wrap(vars(factor), nrow = ncol(Y_t2), dir = &quot;v&quot;, scales = &quot;free&quot;) + labs(title = &quot;Simulated series (two Student t)&quot;, y = &quot;&quot;, x= &quot;&quot;) ## GMM estimation ## Specify the moments u &lt;- rbind(c(2,0),c(0,2),c(1,0),c(2,0),c(1,0),c(2,0),c(0,1),c(0,2),c(0,1),c(0,2)) v &lt;- rbind(c(0,0),c(0,0),c(2,0),c(1,0),c(0,2),c(0,1),c(0,2),c(0,1),c(2,0),c(1,0)) ## Specify the initial values as the true values + randomness param.ini.0_t2 &lt;- c(Model_t2$C,Model_t2$Theta,kappa.3.modif,kappa.4.modif) # add randomness: param.ini.GMM_t2 &lt;- param.ini.0_t2 * (1 + .2 * rnorm(length(param.ini.0_t2))) ## MLE Estimation ## Set initial values (true values+randomness), ## including the parameters of the distribution of the shocks. param.ini.0_t2 &lt;- c(Model_t2$Phi, Model_t2$Theta, Model_t2$C, param.distri.1$theta1,param.distri.2$theta1, param.distri.1$theta2,param.distri.2$theta2, param.distri.1$theta3,param.distri.2$theta3) # add randomness param.ini.MLE_t2 &lt;- param.ini.0_t2 * (1 + .2 * rnorm(length(param.ini.0_t2))) param.ini.MLE_t2[abs(param.ini.MLE_t2)&gt;10^8] &lt;- 0.01 3.2 Estimation The estimation of the parameters requires many computations and requires a bit of execution time. Since we need to repeat each estimation procedure one time for each of the three combinations of the distributions of the shocks, it is particularly convenient (and simple) to use parallel computing. The next two chunks of code compute the 2SLS-GMM and the ML estimator for the three processes. Notice that in the ML estimation we assume that the actual distributions of the shocks is unknown and we use the p.d.f. of a Gaussian mixtures in all of the three cases. This condition better approximates the practical case where the true distribution is unknown. Therefore, the estimation method is more precisely a pseudo maximum likelihood approach. The choice of the Gaussian mixture depends on its parsimony (in terms of number of parameters) and flexibility. Indeed, it admits bimodality and can attain any possible combination of skewness and kurtosis such that \\(\\text{kurtosis} \\geq \\text{skewness}^2 +1\\). ## Preliminary settings for the optimization algorithm nb.loops &lt;- 2 MAXIT.nlminb &lt;- 300 MAXIT.NlMd &lt;- 1000 indic.Blaschke &lt;- 1 # Consider Blaschke transformations of Theta after initial estimation nb.loops.BM &lt;- 2 MAXIT.nlminb.BM &lt;- 300 MAXIT.NlMd.BM &lt;- 1000 indic.solve.C &lt;- 1 # override inapprorpriate C matrix (during optimization) ## GMM estimator data_list &lt;- list(Y_mixt2, Y_mixt_t, Y_t2) initial_list &lt;- list(param.ini.GMM_mixt2, param.ini.GMM_mixt_t, param.ini.GMM_t2) cl &lt;- parallel::makeCluster(3) doParallel::registerDoParallel(cl) results_GMM &lt;- foreach(i = 1:length(data_list), .packages=&#39;optimx&#39;) %dopar%{ estim.VARMAp1.2SLS.GMM(data_list[[i]], u, v, nb.iterations.gmm = 2, maxitNM = 2000, indic.Blaschke = 1, # after the first estimation, transform Theta # via Blaschke matrices to get new starting values # for different regimes of fundamentalness indic.print=0, lag.NW=2, # number of lags used in the Newey-West approach # used to compute standard deviations of parameter estimates indic.estim.phi=1, # estimate also the AR coefficients indic.constant = 0, # determines if the model includes a constant indic.3rd.4th.order.moments=1, # both 3rd-order and 4th-order moments are used in the GMM # if indic.3rd.4th.order.moments==3, then use only third-order moments # if indic.3rd.4th.order.moments ==4, then use only 4th-order moments param.ini=initial_list[[i]], # specify initial values p = p, addit.IV=3 ) } parallel::stopCluster(cl) names(results_GMM) &lt;- c(&quot;mixt2&quot;, &quot;mixt_t&quot;, &quot;t2&quot;) ## Maximum likelihood estimator data_list &lt;- list(Y_mixt2, Y_mixt_t, Y_t2) models_list &lt;- list(Model_mixt2, Model_mixt_t, Model_t2) initial_list &lt;- list(param.ini.MLE_mixt2, param.ini.MLE_mixt_t, param.ini.MLE_t2) cl &lt;- parallel::makeCluster(3) doParallel::registerDoParallel(cl) results_ML &lt;- foreach(i = 1:length(data_list), .packages=c(&#39;optimx&#39;, &#39;Matrix&#39;)) %dopar%{ estim.MA.inversion(initial_list[[i]], models_list[[i]], data_list[[i]], MAXIT.nlminb = MAXIT.nlminb, # max iterations for nlminb optimization MAXIT.NlMd = MAXIT.NlMd, # max iterations in Nelder-Mead optimization nb.loops = nb.loops, indic.Blaschke = 1, # Transform Theta to get different non-fund regimes MAXIT.nlminb.BM = MAXIT.nlminb, # max iter of nlminb after Blaschke transformations MAXIT.NlMd.BM = MAXIT.NlMd, # max iter in Nelder-Mead after Blaschke transformations nb.loops.BM = nb.loops.BM, indic.print=0, # print indic.compute.cov.mat=1 # compute covariance matrix of parameter estimates ) } parallel::stopCluster(cl) names(results_ML) &lt;- c(&quot;mixt2&quot;, &quot;mixt_t&quot;, &quot;t2&quot;) 3.3 Results Below, we report three tables that summarise the results. ## Print results in kables library(knitr) ## Warning: package &#39;knitr&#39; was built under R version 3.6.2 Parameters &lt;- c(&quot;$\\\\Phi_{11}$&quot;, &quot;$\\\\Phi_{21}$&quot;, &quot;$\\\\Phi_{12}$&quot;, &quot;$\\\\Phi_{22}$&quot;, &quot;$\\\\Theta_{11}$&quot;, &quot;$\\\\Theta_{21}$&quot;, &quot;$\\\\Theta_{12}$&quot;, &quot;$\\\\Theta_{22}$&quot;, &quot;$C_{11}$&quot;, &quot;$C_{21}$&quot;, &quot;$C_{12}$&quot;, &quot;$C_{22}$&quot;) 3.3.1 Two Gaussian Mixtures True &lt;- c(Model_mixt2$Phi, Model_mixt2$Theta, Model_mixt2$C) GMM &lt;- c(results_GMM$mixt2$Phi.est, results_GMM$mixt2$Theta.est, results_GMM$mixt2$C0.est) GMM.stdev &lt;- c(sqrt(diag(results_GMM$mixt2$Asympt$Var.alpha.hat)), sqrt(diag(results_GMM$mixt2$Asympt$Var.beta.hat)[5:8]), sqrt(diag(results_GMM$mixt2$Asympt$Var.beta.hat)[1:4])) MLE &lt;- c(results_ML$mixt2$Phi, results_ML$mixt2$Theta, results_ML$mixt2$C) MLE.stdv &lt;- sqrt(diag(results_ML$mixt2$MV)[1:12]) tibble(Parameters, True,GMM,GMM.stdev,MLE,MLE.stdv) %&gt;% mutate_if(is.numeric, format, digits=3) %&gt;% kable(caption = &quot;Estimates of the parameters. The structural shocks are both distributed as Gaussian mixtures&quot;) Table 3.1: Estimates of the parameters. The structural shocks are both distributed as Gaussian mixtures Parameters True GMM GMM.stdev MLE MLE.stdv \\(\\Phi_{11}\\) 0.8 0.7874 0.0352 0.7942 0.00661 \\(\\Phi_{21}\\) -0.3 -0.2639 0.0653 -0.3198 0.03229 \\(\\Phi_{12}\\) 0.3 0.3453 0.0439 0.2931 0.00689 \\(\\Phi_{22}\\) 0.5 0.5667 0.0839 0.5209 0.02970 \\(\\Theta_{11}\\) -0.5 -0.4166 0.1248 -0.5048 0.01798 \\(\\Theta_{21}\\) 1.0 0.3032 3.2928 0.9425 0.14850 \\(\\Theta_{12}\\) 0.0 -0.0960 1.3531 0.0184 0.01583 \\(\\Theta_{22}\\) -2.0 -2.6837 1.0324 -2.0761 0.13627 \\(C_{11}\\) 0.0 -0.0296 0.0675 -0.0226 0.01339 \\(C_{21}\\) 1.0 -0.5974 0.2431 0.9123 0.08143 \\(C_{12}\\) 1.0 0.9291 0.0741 0.9555 0.04064 \\(C_{22}\\) 0.5 0.0155 1.1746 0.4202 0.07429 3.3.2 Gaussian mixtures and t-student True &lt;- c(Model_mixt_t$Phi, Model_mixt_t$Theta, Model_mixt_t$C) GMM &lt;- c(results_GMM$mixt_t$Phi.est, results_GMM$mixt_t$Theta.est, results_GMM$mixt_t$C0.est) GMM.stdev &lt;- c(sqrt(diag(results_GMM$mixt_t$Asympt$Var.alpha.hat)), sqrt(diag(results_GMM$mixt_t$Asympt$Var.beta.hat)[5:8]), sqrt(diag(results_GMM$mixt_t$Asympt$Var.beta.hat)[1:4])) MLE &lt;- c(results_ML$mixt_t$Phi, results_ML$mixt_t$Theta, results_ML$mixt_t$C) MLE.stdv &lt;- sqrt(diag(results_ML$mixt_t$MV)[1:12]) tibble(Parameters, True,GMM,GMM.stdev,MLE,MLE.stdv) %&gt;% mutate_if(is.numeric, format, digits=3) %&gt;% kable(caption = &quot;Estimates of the parameters. Shock 1 is distributed as a Gaussian mixture, Shock 2 is distributed as a Student-t&quot;) Table 3.2: Estimates of the parameters. Shock 1 is distributed as a Gaussian mixture, Shock 2 is distributed as a Student-t Parameters True GMM GMM.stdev MLE MLE.stdv \\(\\Phi_{11}\\) 0.8 0.79387 0.0443 0.80571 0.0249 \\(\\Phi_{21}\\) -0.3 -0.21871 0.0803 -0.28341 0.0360 \\(\\Phi_{12}\\) 0.3 0.30281 0.0368 0.27944 0.0204 \\(\\Phi_{22}\\) 0.5 0.60563 0.0690 0.48775 0.0340 \\(\\Theta_{11}\\) -0.5 -0.32365 0.2307 -0.42512 0.0549 \\(\\Theta_{21}\\) 1.0 1.83036 0.7210 0.96220 0.1600 \\(\\Theta_{12}\\) 0.0 0.00764 0.3047 -0.05963 0.0570 \\(\\Theta_{22}\\) -2.0 -2.28968 0.6047 -1.92401 0.1336 \\(C_{11}\\) 0.0 -0.08657 0.0925 -0.00305 0.0649 \\(C_{21}\\) 1.0 0.63183 0.1317 1.04883 0.0924 \\(C_{12}\\) 1.0 0.84969 0.0434 0.94151 0.0518 \\(C_{22}\\) 0.5 0.64782 0.3537 0.42375 0.0864 3.3.3 Two t-student True &lt;- c(Model_t2$Phi, Model_t2$Theta, Model_t2$C) GMM &lt;- c(results_GMM$t2$Phi.est, results_GMM$t2$Theta.est, results_GMM$t2$C0.est) GMM.stdev &lt;- c(sqrt(diag(results_GMM$t2$Asympt$Var.alpha.hat)), sqrt(diag(results_GMM$t2$Asympt$Var.beta.hat)[5:8]), sqrt(diag(results_GMM$t2$Asympt$Var.beta.hat)[1:4])) MLE &lt;- c(results_ML$t2$Phi, results_ML$t2$Theta, results_ML$t2$C) MLE.stdv &lt;- sqrt(diag(results_ML$t2$MV)[1:12]) tibble(Parameters, True,GMM,GMM.stdev,MLE,MLE.stdv) %&gt;% mutate_if(is.numeric, format, digits=3) %&gt;% kable(caption = &quot;Estimates of the parameters. The structural shocks are both distributed as mixture Gaussians.&quot;) Table 3.3: Estimates of the parameters. The structural shocks are both distributed as mixture Gaussians. Parameters True GMM GMM.stdev MLE MLE.stdv \\(\\Phi_{11}\\) 0.8 0.808 0.0374 0.8207 0.0153 \\(\\Phi_{21}\\) -0.3 -0.392 0.0653 -0.3749 0.0405 \\(\\Phi_{12}\\) 0.3 0.315 0.0400 0.3154 0.0121 \\(\\Phi_{22}\\) 0.5 0.537 0.0608 0.5730 0.0399 \\(\\Theta_{11}\\) -0.5 -2.179 0.5948 -0.5994 0.0149 \\(\\Theta_{21}\\) 1.0 -1.594 0.8446 0.8510 0.0826 \\(\\Theta_{12}\\) 0.0 -0.583 0.2113 0.0463 0.0138 \\(\\Theta_{22}\\) -2.0 -1.855 0.4369 -1.5144 0.0667 \\(C_{11}\\) 0.0 0.289 0.0962 -0.4091 0.0277 \\(C_{21}\\) 1.0 0.473 0.1664 0.8530 0.0692 \\(C_{12}\\) 1.0 0.406 0.1054 0.9764 0.0500 \\(C_{22}\\) 0.5 -0.973 0.2613 1.0192 0.0815 As we can see, the GMM estimator approximates the true value of the distribution only when the distribution of the shocks are sufficiently different from the Gaussian case. However, the 2SLS-GMM estimator is significantly less precise in all the three cases. For this reason, in practice, GMR2019 suggest to use this estimator to choose the initial values of the (pseudo) maximum likelihood procedure. The latter estimator is indeed more efficient and, in our example, performs sufficiently well in estimating the parameters, also in presence of non-fundamentalness. However, the case of two Student t distribution leads to unstable estimates, i.e. the estimated values and fundamentalness regimes can change substantially in different random simulations of the model. On the contrary, the estimates are stable in the other two cases. "],["application-to-real-world-data.html", "Section 4 Application to real-world data", " Section 4 Application to real-world data In this section, we implement the GMR2019 method on the FRED-MD dataset. We take inspiration from the bivariate VAR model by Blanchard and Quah (1989), by fitting a bivariate VARMA(4,1) model of the growth of industrial production and the unemployment rate using the monthly data for the U.S. economy over the period 1960-2020. This naive model will still prove to be useful to get a practical sense of the procedure. As a first step, we import the data and select the relevant series. library(BVAR) ## Used only to call the fred-md dataset ## ## Attaching package: &#39;BVAR&#39; ## The following objects are masked from &#39;package:vars&#39;: ## ## fevd, irf library(lubridate) ## ## Attaching package: &#39;lubridate&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## date, intersect, setdiff, union source(&quot;C:/Users/Alessandro Ciancetta/Documents/Universita/UniPi/V/TesiEconomics/ScriptR-TesiEconomics/DaFare26maggio/Code/sources/FAVAR_functions/plot_ts.R&quot;) data &lt;- fred_md %&gt;% rownames_to_column(var = &quot;date&quot;) %&gt;% mutate(date = as_date(date)) %&gt;% # select(date, INDPRO, CPIAUCSL, FEDFUNDS, UNRATE) %&gt;% select(date, INDPRO, UNRATE) %&gt;% filter(date &gt;= ymd(19600101) &amp; date &lt; ymd(20200101)) dates &lt;- data$date data &lt;- data[,-1] plot_ts(data, date = dates) We then pre-process the data to get stationary time series, by taking the growth rate of the industrial production and the de-trended unemployment rate. T &lt;- nrow(data) ## growth rates of the industrial production indpro_growth &lt;- data$INDPRO/(c(NaN,data$INDPRO[1:(T-1)])) - 1 ## De-trended rate of unemployment ## (in this case, this almost coincide with de-meaning) trend &lt;- 1:T unrate_detrend &lt;- lm(data$UNRATE ~ trend)$residuals # View(cbind(data$UNRATE, unrate_detrend, data$UNRATE-unrate_detrend)) y &lt;- tibble(indpro_growth=indpro_growth[-1], unrate=unrate_detrend[-1]) plot_ts(y) We can now estimate the model. As a first step, we compute the 2SLS-GMM estimator. This method is quite imprecise, but it has the advantage of requiring no assumptions on the distribution of the errors and it is faster than the maximum likelihood approach. For this reason, GMR2019 suggest to use this estimator as a first step, and to then use the estiamted values as the initial parameters of for the ML procedure. Since the true distribution of the shocks is unknown, we more precisely adopt a pseudo maximum likelihood approach by considering the p.d.f. of a Gaussian mixture. ## Set the number of lags p = 4 ## Initialize the cumulants (for GMM) df &lt;- 5 mu.2 &lt;- 1 mu.4 &lt;- (3 + 6/(df - 4)) mu.3 &lt;- 0 kappa.3.modif &lt;- 0 kappa.4.modif &lt;- mu.4 - 3*mu.2^2 ## Overweight order-2 moments in the first step multipl.order.2 &lt;- 10 indic.solve.C &lt;- 1 #0 u &lt;- rbind( c(2,0), c(0,2), c(1,0), c(2,0), c(1,0), c(2,0), c(0,1), c(0,2), c(0,1), c(0,2) ) v &lt;- rbind( c(0,0), c(0,0), c(2,0), c(1,0), c(0,2), c(0,1), c(0,2), c(0,1), c(2,0), c(1,0) ) param.ini.4.GMM &lt;- NULL ## Estimation res.estim.GMM &lt;- estim.VARMAp1.2SLS.GMM(as.matrix(y),u,v, nb.iterations.gmm = 5, maxitNM=2000, indic.Blaschke=1, indic.print=0, lag.NW=3, indic.estim.phi=1, indic.constant = 1, indic.3rd.4th.order.moments=1, param.ini=param.ini.4.GMM, p=p, addit.IV=3 ## number of IVs ) ## Warning in optimx.check(par, optcfg$ufn, optcfg$ugr, optcfg$uhess, lower, : Parameters or bounds appear to have different scalings. ## This can cause poor performance in optimization. ## It is important for derivative free methods like BOBYQA, UOBYQA, NEWUOA. ## [1] &quot;=========================================================&quot; ## [1] &quot;=========================================================&quot; ## [1] &quot; First pass: &#39;Ad hoc&#39; Omega &quot; ## [1] &quot;=========================================================&quot; ## [1] &quot;=========================================================&quot; ## [1] &quot;--- Initial value of the criteria: 29937.6155367797&quot; ## [1] &quot;--- Starting Minimization of criteria ---&quot; ## [1] &quot;Criteria at the end of iteration 1 (Max eval:2000): 40.1708158013186&quot; ## [1] &quot;Criteria at the end of iteration 2 (Max eval:2000): 27.5107882558548&quot; ## [1] &quot;Criteria at the end of iteration 3 (Max eval:2000): 26.9856472785477&quot; ## [1] &quot;Criteria at the end of iteration 4 (Max eval:2000): 26.8099561388832&quot; ## [1] &quot;Criteria at the end of iteration 5 (Max eval:2000): 26.7317391216593&quot; ## [1] &quot;--- End of Minimization of criteria ---&quot; ## [,1] [,2] ## [1,] -0.3803227 0.02150216 ## [2,] 2.2840275 0.68875288 ## [1] &quot;Looking for alternative MA representation (1/4)&quot; ## [1] &quot;Looking for alternative MA representation (2/4)&quot; ## [1] &quot;Looking for alternative MA representation (3/4)&quot; ## [1] &quot;Looking for alternative MA representation (4/4)&quot; ## [1] &quot;--- Initial value of the criteria: 183.517926644411&quot; ## [1] &quot;--- Starting Minimization of criteria ---&quot; ## [1] &quot;Criteria at the end of iteration 1 (Max eval:2000): 28.805434800134&quot; ## [1] &quot;Criteria at the end of iteration 2 (Max eval:2000): 28.6132274326595&quot; ## [1] &quot;Criteria at the end of iteration 3 (Max eval:2000): 28.5239740250049&quot; ## [1] &quot;Criteria at the end of iteration 4 (Max eval:2000): 28.5022171426145&quot; ## [1] &quot;Criteria at the end of iteration 5 (Max eval:2000): 28.5015763825543&quot; ## [1] &quot;--- End of Minimization of criteria ---&quot; ## [1] &quot;--- Initial value of the criteria: 164.411475847416&quot; ## [1] &quot;--- Starting Minimization of criteria ---&quot; ## [1] &quot;Criteria at the end of iteration 1 (Max eval:2000): 22.338224897635&quot; ## [1] &quot;Criteria at the end of iteration 2 (Max eval:2000): 21.9489708511242&quot; ## [1] &quot;Criteria at the end of iteration 3 (Max eval:2000): 21.4215159195505&quot; ## [1] &quot;Criteria at the end of iteration 4 (Max eval:2000): 21.0532296111802&quot; ## [1] &quot;Criteria at the end of iteration 5 (Max eval:2000): 20.7530964006408&quot; ## [1] &quot;--- End of Minimization of criteria ---&quot; ## [1] &quot;&quot; ## [1] &quot; ++++ Improvement of criteria ++++&quot; ## [1] &quot;&quot; ## [,1] [,2] ## [1,] -0.256607 0.03564682 ## [2,] 10.328807 1.31278123 ## [1] &quot;--- Initial value of the criteria: 583.592122111067&quot; ## [1] &quot;--- Starting Minimization of criteria ---&quot; ## [1] &quot;Criteria at the end of iteration 1 (Max eval:2000): 32.1926192844288&quot; ## [1] &quot;Criteria at the end of iteration 2 (Max eval:2000): 31.6598609210212&quot; ## [1] &quot;Criteria at the end of iteration 3 (Max eval:2000): 31.1925827317296&quot; ## [1] &quot;Criteria at the end of iteration 4 (Max eval:2000): 31.1190938169463&quot; ## [1] &quot;Criteria at the end of iteration 5 (Max eval:2000): 31.0682874150904&quot; ## [1] &quot;--- End of Minimization of criteria ---&quot; ## [1] &quot;=========================================================&quot; ## [1] &quot;=========================================================&quot; ## [1] &quot; Second pass: &#39;Optimal&#39; Omega &quot; ## [1] &quot;=========================================================&quot; ## [1] &quot;=========================================================&quot; ## [1] &quot;Criteria at the end of iteration 1 (Max eval:2000): 13.3410870057891&quot; ## [1] &quot;Criteria at the end of iteration 2 (Max eval:2000): 12.9875604677233&quot; ## [1] &quot;Criteria at the end of iteration 3 (Max eval:2000): 12.8917352487576&quot; ## [1] &quot;Criteria at the end of iteration 4 (Max eval:2000): 12.8822959720433&quot; ## [1] &quot;Criteria at the end of iteration 5 (Max eval:2000): 12.679107536186&quot; ## [1] &quot;--- End of Minimization of criteria ---&quot; res.estim.GMM$Theta ## [,1] [,2] ## [1,] -0.1349984 0.02815548 ## [2,] 8.9776912 1.16963120 ## Set the GMM estimates as the initial values for the ML ## optimization procedure Model.est.GMM &lt;- list() Model.est.GMM$Phi &lt;- res.estim.GMM$Phi.est n &lt;- dim(Model.est.GMM$Phi)[1] Model.est.GMM$C &lt;- res.estim.GMM$C0.est Model.est.GMM$Theta &lt;- array(res.estim.GMM$Theta.est,c(n,n,1)) Model.est.GMM$distri &lt;- NULL Model.est.GMM$Mu &lt;- res.estim.GMM$mu.est ETA.est &lt;- estim.struct.shocks(Y = as.matrix(y), Model = Model.est.GMM)$ETA.est vec.theta1 &lt;- NULL vec.theta2 &lt;- NULL vec.theta3 &lt;- NULL par(mfrow=c(2,2)) for(i in 1:n){ res.estim &lt;- estim.MLE.mixtures(ETA.est[,i]) vec.theta1 &lt;- c(vec.theta1,res.estim$param.est[1]) vec.theta2 &lt;- c(vec.theta2,res.estim$param.est[2]) vec.theta3 &lt;- c(vec.theta3,res.estim$param.est[3]) xx &lt;- seq(-10,10,by=.01) f &lt;- density.mixt.gauss(res.estim$param.est,xx) plot(xx,f,type=&quot;l&quot;) lines(density(ETA.est[,i],bw=.1),col=&quot;red&quot;) } ## Estimate the model param.ini &lt;- c(res.estim.GMM$Phi.est,res.estim.GMM$Theta.est, res.estim.GMM$C0.est, vec.theta1,vec.theta2,vec.theta3) res.estim.MLE &lt;- estim.MA.inversion(param.ini,Model.est.GMM,as.matrix(y), MAXIT.nlminb=300, MAXIT.NlMd=1000, nb.loops=4, indic.Blaschke=1, MAXIT.nlminb.BM=300, MAXIT.NlMd.BM=1000, nb.loops.BM=2, indic.print=0,indic.compute.cov.mat=1) ## [1] &quot;param.ini:&quot; ## [1] -2.270024e-01 -3.869033e+00 1.389561e-02 1.545562e+00 2.687431e-01 ## [6] 1.404742e+00 -1.818062e-02 -3.712492e-01 2.056953e-01 1.057485e-02 ## [11] -3.840567e-05 -1.599325e-01 1.786540e-01 -8.262147e-01 4.901788e-03 ## [16] -1.902981e-02 -1.349984e-01 8.977691e+00 2.815548e-02 1.169631e+00 ## [21] 8.939772e-04 9.557083e-02 -5.914673e-03 7.693439e-02 5.508629e-02 ## [26] 2.368594e+00 -7.585889e-01 -6.646727e-02 5.313747e-02 -9.038630e-02 ## Warning in optimx.check(par, optcfg$ufn, optcfg$ugr, optcfg$uhess, lower, : Parameters or bounds appear to have different scalings. ## This can cause poor performance in optimization. ## It is important for derivative free methods like BOBYQA, UOBYQA, NEWUOA. ## Warning in optimx.check(par, optcfg$ufn, optcfg$ugr, optcfg$uhess, lower, : Parameters or bounds appear to have different scalings. ## This can cause poor performance in optimization. ## It is important for derivative free methods like BOBYQA, UOBYQA, NEWUOA. ## Warning in optimx.check(par, optcfg$ufn, optcfg$ugr, optcfg$uhess, lower, : Parameters or bounds appear to have different scalings. ## This can cause poor performance in optimization. ## It is important for derivative free methods like BOBYQA, UOBYQA, NEWUOA. ## Warning in optimx.check(par, optcfg$ufn, optcfg$ugr, optcfg$uhess, lower, : Parameters or bounds appear to have different scalings. ## This can cause poor performance in optimization. ## It is important for derivative free methods like BOBYQA, UOBYQA, NEWUOA. ## [1] &quot;Looking for alternative MA representation (1/4)&quot; ## [1] &quot;Looking for alternative MA representation (2/4)&quot; ## [1] &quot;Looking for alternative MA representation (3/4)&quot; ## [1] &quot;Looking for alternative MA representation (4/4)&quot; ## [1] &quot;param.ini.i:&quot; ## [1] -2.270024e-01 -3.869033e+00 1.389561e-02 1.545562e+00 2.687431e-01 ## [6] 1.404742e+00 -1.818062e-02 -3.712492e-01 2.056953e-01 1.057485e-02 ## [11] -3.840567e-05 -1.599325e-01 1.786540e-01 -8.262147e-01 4.901788e-03 ## [16] -1.902981e-02 -4.358196e+00 1.213685e+01 -2.493017e-02 7.251465e-01 ## [21] -1.552182e-03 3.750390e-03 -1.200788e-03 1.555104e-01 5.508629e-02 ## [26] 2.368594e+00 -7.585889e-01 -6.646727e-02 5.313747e-02 -9.038630e-02 ## Warning in optimx.check(par, optcfg$ufn, optcfg$ugr, optcfg$uhess, lower, : Parameters or bounds appear to have different scalings. ## This can cause poor performance in optimization. ## It is important for derivative free methods like BOBYQA, UOBYQA, NEWUOA. ## Warning in optimx.check(par, optcfg$ufn, optcfg$ugr, optcfg$uhess, lower, : Parameters or bounds appear to have different scalings. ## This can cause poor performance in optimization. ## It is important for derivative free methods like BOBYQA, UOBYQA, NEWUOA. ## [1] &quot;param.ini.i:&quot; ## [1] -2.270024e-01 -3.869033e+00 1.389561e-02 1.545562e+00 2.687431e-01 ## [6] 1.404742e+00 -1.818062e-02 -3.712492e-01 2.056953e-01 1.057485e-02 ## [11] -3.840567e-05 -1.599325e-01 1.786540e-01 -8.262147e-01 4.901788e-03 ## [16] -1.902981e-02 -2.156171e-01 2.127534e+00 7.057141e-03 6.478586e-01 ## [21] 1.129001e-03 1.408667e-01 -6.683738e-03 6.782243e-02 5.508629e-02 ## [26] 2.368594e+00 -7.585889e-01 -6.646727e-02 5.313747e-02 -9.038630e-02 ## Warning in optimx.check(par, optcfg$ufn, optcfg$ugr, optcfg$uhess, lower, : Parameters or bounds appear to have different scalings. ## This can cause poor performance in optimization. ## It is important for derivative free methods like BOBYQA, UOBYQA, NEWUOA. ## Warning in optimx.check(par, optcfg$ufn, optcfg$ugr, optcfg$uhess, lower, : Parameters or bounds appear to have different scalings. ## This can cause poor performance in optimization. ## It is important for derivative free methods like BOBYQA, UOBYQA, NEWUOA. ## [1] &quot;param.ini.i:&quot; ## [1] -2.270024e-01 -3.869033e+00 1.389561e-02 1.545562e+00 2.687431e-01 ## [6] 1.404742e+00 -1.818062e-02 -3.712492e-01 2.056953e-01 1.057485e-02 ## [11] -3.840567e-05 -1.599325e-01 1.786540e-01 -8.262147e-01 4.901788e-03 ## [16] -1.902981e-02 -4.477190e+00 3.611265e+01 -2.968505e-02 1.683193e+00 ## [21] -1.544659e-03 9.326253e-03 -1.077550e-03 1.084715e-01 5.508629e-02 ## [26] 2.368594e+00 -7.585889e-01 -6.646727e-02 5.313747e-02 -9.038630e-02 ## Warning in optimx.check(par, optcfg$ufn, optcfg$ugr, optcfg$uhess, lower, : Parameters or bounds appear to have different scalings. ## This can cause poor performance in optimization. ## It is important for derivative free methods like BOBYQA, UOBYQA, NEWUOA. ## Warning in optimx.check(par, optcfg$ufn, optcfg$ugr, optcfg$uhess, lower, : Parameters or bounds appear to have different scalings. ## This can cause poor performance in optimization. ## It is important for derivative free methods like BOBYQA, UOBYQA, NEWUOA. res.estim.MLE$Theta ## [,1] [,2] ## [1,] 0.09898047 0.0244470 ## [2,] 2.67743894 0.5995404 The next table compares the results of the estimation. Parameters &lt;- c(&quot;$\\\\Phi_{11}^{(1)}$&quot;, &quot;$\\\\Phi_{21}^{(1)}$&quot;, &quot;$\\\\Phi_{12}^{(1)}$&quot;, &quot;$\\\\Phi_{22}^{(1)}$&quot;, &quot;$\\\\Phi_{11}^{(2)}$&quot;, &quot;$\\\\Phi_{21}^{(2)}$&quot;, &quot;$\\\\Phi_{12}^{(2)}$&quot;, &quot;$\\\\Phi_{22}^{(2)}$&quot;, &quot;$\\\\Phi_{11}^{(3)}$&quot;, &quot;$\\\\Phi_{21}^{(3)}$&quot;, &quot;$\\\\Phi_{12}^{(3)}$&quot;, &quot;$\\\\Phi_{22}^{(3)}$&quot;, &quot;$\\\\Phi_{11}^{(4)}$&quot;, &quot;$\\\\Phi_{21}^{(4)}$&quot;, &quot;$\\\\Phi_{12}^{(4)}$&quot;, &quot;$\\\\Phi_{22}^{(4)}$&quot;, &quot;$\\\\Theta_{11}$&quot;, &quot;$\\\\Theta_{21}$&quot;, &quot;$\\\\Theta_{12}$&quot;, &quot;$\\\\Theta_{22}$&quot;, &quot;$C_{11}$&quot;, &quot;$C_{21}$&quot;, &quot;$C_{12}$&quot;, &quot;$C_{22}$&quot;) GMM &lt;- c(res.estim.GMM$Phi.est, res.estim.GMM$Theta, res.estim.GMM$C0.est) GMM.stdev &lt;- c(sqrt(diag(res.estim.GMM$Asympt$Var.alpha.hat))[3:18], sqrt(diag(res.estim.GMM$Asympt$Var.beta.hat)[5:8]), sqrt(diag(res.estim.GMM$Asympt$Var.beta.hat)[1:4])) MLE &lt;- c(res.estim.MLE$Phi, res.estim.MLE$Theta, res.estim.MLE$C) MLE.stdv &lt;- sqrt(diag(res.estim.MLE$MV)[1:24]) tibble(Parameters, GMM,GMM.stdev ,MLE,MLE.stdv) %&gt;% mutate_if(is.numeric, round, digits=3) %&gt;% knitr::kable(caption = &quot;Estimates of the parameters&quot;) Table 4.1: Estimates of the parameters Parameters GMM GMM.stdev MLE MLE.stdv \\(\\Phi_{11}^{(1)}\\) -0.227 0.183 0.258 0.111 \\(\\Phi_{21}^{(1)}\\) -3.869 6.589 -3.276 1.379 \\(\\Phi_{12}^{(1)}\\) 0.014 0.008 0.019 0.008 \\(\\Phi_{22}^{(1)}\\) 1.546 0.333 1.453 0.071 \\(\\Phi_{11}^{(2)}\\) 0.269 0.079 0.274 0.073 \\(\\Phi_{21}^{(2)}\\) 1.405 2.697 0.945 0.961 \\(\\Phi_{12}^{(2)}\\) -0.018 0.008 -0.019 0.008 \\(\\Phi_{22}^{(2)}\\) -0.371 0.301 -0.276 0.091 \\(\\Phi_{11}^{(3)}\\) 0.206 0.054 0.182 0.056 \\(\\Phi_{21}^{(3)}\\) 0.011 1.551 0.228 1.076 \\(\\Phi_{12}^{(3)}\\) 0.000 0.003 -0.004 0.003 \\(\\Phi_{22}^{(3)}\\) -0.160 0.086 -0.060 0.062 \\(\\Phi_{11}^{(4)}\\) 0.179 0.050 0.114 0.056 \\(\\Phi_{21}^{(4)}\\) -0.826 1.486 0.423 0.797 \\(\\Phi_{12}^{(4)}\\) 0.005 0.002 0.004 0.002 \\(\\Phi_{22}^{(4)}\\) -0.019 0.063 -0.123 0.032 \\(\\Theta_{11}\\) -0.135 0.167 0.099 0.105 \\(\\Theta_{21}\\) 8.978 4.204 2.677 1.090 \\(\\Theta_{12}\\) 0.028 0.014 0.024 0.008 \\(\\Theta_{22}\\) 1.170 0.403 0.600 0.067 \\(C_{11}\\) 0.001 0.001 0.000 0.001 \\(C_{21}\\) 0.096 0.028 0.150 0.007 \\(C_{12}\\) -0.006 0.001 -0.007 0.000 \\(C_{22}\\) 0.077 0.024 0.036 0.029 "]]
