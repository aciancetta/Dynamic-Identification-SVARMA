[["index.html", "Dynamic Identification in VARMA Models Abstract", " Dynamic Identification in VARMA Models Alessandro Ciancetta 2021-07-13 Abstract This book present the general framework of VARMA models and introduces the notion of non-fundamentalness. The problem is assessed using the methodology of Gourieroux, Manfort and Renne (2019), which is illustrated in detail, within the framework of non-Gaussian and independent structural shocks of the corresponding structural model. Simulations and application to real-world data are presented. "],["introduction.html", "Section 1 Introduction 1.1 Assumptions for SSVARMA models 1.2 Invertibility of the MA part 1.3 Non-fundamentalness", " Section 1 Introduction VARMA models are an interesting generalization of ARMA models for multivariate time series. This class of models take into account the (direct) lagged effect that a shock can have on the system under investigation. This could be useful in practice, for example whenever we think that the process might react to the shocks also after some delay (i.e. if the error term has an MA structure). However, VARMA models are not as popular as standard VAR models, also because of the difficulties of estimating these models when compared with the simple least-squares estimation which is common in the VAR literature. The features of these models, however, may reveal crucial also for structural-VAR modeling. In particular, a VAR model can be seen as an approximation of an underlying VARMA process, but only if certain conditions on the moving-average parameters hold. Basically these conditions states that the parameters of the delayed effect of the shock should be smaller than one in modulus: that is, the shocks should not have an explosive lagged impact on the system. This condition - which we will make more precise throughout this essay - allows for the invertibility of the MA part of the model, thus making VAR approximations possible. However, in practice one does not know a priori whether the condition hold or not, i.e. whether the shocks are respectively fundamental or not-fundamental. In the latter case, the process cannot be inverted in its past values and an autoregressive representation of the model does not exist: a VAR representation, since it fails to identify the true parameters, would then be wrong, and the estimated impulse response functions would not be those associated with the true non-fundamental process. Therefore, if we have some reasons to believe that the process that we are estimating has a VARMA structure, we should keep in mind that VAR modeling could be misleading. In this essay, we discuss the methodology proposed by Gourieroux, Monfort and Renne (2019, GMR2019 henceforth) to assess the problem of dynamic identification of non-fundamental SVARMA processes. Section 1 presents the main assumptions and provides a formal definition of non-fundamentalness. Section 2 presents the method in detail. Section 3 discusses a simple simulation example in the case of a bivariate model. Section 4 is a toy-example of applications to real-world data. 1.1 Assumptions for SSVARMA models Consider the following VARMA model: \\[ \\Phi(L) Y_t = \\Theta(L) \\varepsilon_t, \\quad Y_t \\in \\mathbb{R}^n \\] where \\[ \\Phi(L) = I_n - \\Phi_1L - \\dots - \\Phi_p L^p \\] \\[ \\Theta(L) = I_n - \\Theta_1L - \\dots - \\Theta_q L^q \\] We want to make the following assumptions: Hp. 1 (Independent shocks). There exists a linear transformation such that the (correlated) VARMA residuals can be expressed as the transformation of a vector of structural shocks that are mutually independent, that is: \\[ \\exists \\ C \\in M_{n\\times n} \\ | \\ \\varepsilon_t = C \\eta_t \\quad \\text{with } \\det C \\neq 0, \\] where \\(\\eta_t \\in \\mathbb{R}^n\\) is such that \\((\\eta_1, \\eta_2, \\dots , \\eta_T)\\) are independently and identically distributed (so that the shocks at each time period are random draws from the same joint distribution) and \\((\\eta_{1t}, \\eta_{2t}, \\dots, \\eta_{nt})\\) are mutually independent (i.e. the joint distribution of the shocks is the product of their marginal distributions). \\(\\eta_t\\) are called the structural shocks of the process. The condition \\(\\det C \\neq 0\\) means that the transformation has full rank, and therefore that both \\(\\varepsilon_t\\) and \\(\\eta_t\\) belongs to \\(\\mathbb{R}^n\\) and the number of structural shocks equals the number of variables in the system. Also, it implies the existence of \\(C^{-1}\\). When Hp.1 holds, the process is said to be a strong-structural VARMA model (SSVARMA), where strong derives from the assumption of mutual independence of the simultaneous shocks. Hp. 2 (Minimum order representation). Matrices \\(\\Phi(L), \\ \\Theta(L)\\) are left-comprime, i.e. if there exists a common matrix \\(C(L)\\) such that \\[ \\begin{aligned} \\Phi(L) = C(L) \\widetilde{\\Phi}(L) \\\\ \\Theta(L) = C(L) \\widetilde{\\Theta}(L) \\end{aligned} \\qquad \\text{then } \\det C(L) \\text{ does not depend on L} \\] Since \\(\\det \\Phi(L) = \\det C(L) \\ \\det \\widetilde{\\Phi}(L)\\) and \\(C(L)\\) does not depend on L, Hp.2 implies that \\(\\det \\Phi(L)\\) and \\(\\det \\widetilde{\\Phi}(L)\\) have the same lag order (the same holds for \\(\\det \\Theta(L)\\) and \\(\\det \\widetilde{\\Theta}(L)\\)). Therefore, simplifying \\[ C(L) \\widetilde{\\Phi}(L) = C(L) \\widetilde{\\Theta}(L) \\varepsilon_t \\] by left-multiplying \\(C(L)^{-1}\\) on both sides of the equation would not lead to a reduction of the lag order. This means that both the AR and MA components have minimal order. Hp.3 (Stationarity of the AR component). \\(\\det \\Phi (z) \\neq 0, \\ \\forall z \\in \\mathbb{C} \\ \\text{s.t.} \\ |z| &lt; 1\\), i.e. the determinantal polynomial has roots outside the unit circle. To understand this condition, we have to work on the relationship between the roots of \\(\\Theta(z)\\) and the eigenvalues of the companion matrix. Indeed, stationarity is guaranteed when the eigenvalues of the companion matrix are inside the unite circle. A VARMA(p,q) process has the following VARMA(1, q) representation: \\[ \\mathbf{y_t} = \\mathbf{F y_{t-1}} + \\mathbf{v_t} \\] with \\[\\begin{equation} \\mathbf{F} =\\begin{bmatrix} \\Phi_1 &amp; \\Phi_2 &amp; \\dots &amp; \\Phi_{p-1} &amp; \\Phi_p \\\\ I_n &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; I_n &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; I_n &amp; 0 \\end{bmatrix}, \\qquad \\mathbf{y_t} =\\begin{bmatrix} Y_t \\\\ Y_{t-1} \\\\ \\vdots \\\\ Y_{t-p} \\end{bmatrix}, \\qquad \\mathbf{v_t} =\\begin{bmatrix} \\Theta(L)\\varepsilon_t \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\end{equation}\\] Therefore, we can obtain the expression of the \\(h\\)-steps ahead observation as \\[ \\mathbf{y_{t+h}} = \\mathbf{v_{t+h}} + \\mathbf{F v_{t+h-1}} + \\mathbf{F^2 v_{t+h-2}} + \\dots + \\mathbf{F^{h-1} v_{t+1}} + \\mathbf{F^h y_{t}}, \\] from which it is readily seen that, in order for the process to be stationary, the eigenvalues of \\(\\mathbf{F}\\) must lay inside the unit circle (i.e. the effect of each \\(\\mathbf{v_t}\\) must die out). According to Proposition 10.1 in Hamilton (1994), the eigenvalues of \\(\\mathbf{F}\\) satisfy \\[\\begin{equation} \\det(I_n \\lambda^p - \\Phi_1\\lambda^{p-1} - \\dots - \\Phi_{p-1}\\lambda - \\Phi_p) = 0 \\tag{1.1} \\end{equation}\\] We can show that the statement in Hp.3 is just an alternative writing for: the solutions of \\(\\det(I_n \\lambda^p - \\Phi_1\\lambda^{p-1} - \\dots - \\Phi_{p-1}\\lambda - \\Phi_p) = 0\\) (i.e. the eigenvalues of \\(\\mathbf{F}\\)) must be smaller than one in modulus. Indeed, we can factor the polynomial operator \\(\\Phi(L)\\) as: \\[ I - \\Phi_1L - \\Phi_2L^2 - \\dots - \\Phi_p L^p = (I-\\lambda_1 I L) (I-\\lambda_2 I L) \\cdots(I-\\lambda_p I L) \\] The two sides of the previous equation are equal if and only if the following equality holds \\(\\forall z \\in \\mathbb{C}\\): \\[ I - \\Phi_1 z - \\Phi_2 z^2 - \\dots - \\Phi_p z^p = (I-\\lambda_1 I z) (I-\\lambda_2 I z) \\cdots(I-\\lambda_p I z) \\] We can divide both sides by \\(z^p\\) and take the determinant to obtain an expression similar to eq. (1.1) on the left-hand side: \\[\\begin{equation} \\det(I z^{-p} - \\Phi_1 z^{-p+1} - \\dots - \\Phi_{p-1} z^{-1} - \\Phi_p) \\tag{1.2} \\end{equation}\\] Polynomials in (1.1) and (1.2) are equal if and only their roots are \\(z_i = \\lambda_i^{-1}, \\ \\ i \\in \\{1, \\dots, p\\}\\). This entails that the eigenvalues of \\(\\mathbf{F}\\) are inside the unit circle if and only if the roots of the polynomial (1.2) are outside the unit circle. Basically, this equivalence comes from the different representations of the VARMA process as a difference equation or as an equation in the lag operator. Therefore, the condition in Hp.3 is a necessary and sufficient condition for the stationarity of the autoregressive part of the process. Hp. 4 (No unit roots in the MA part). \\(\\det \\Theta(z) \\neq 0, \\ \\forall z \\in \\mathbb{C} \\text{ s.t } |z| = 1\\). This condition: guarantees the generalized invertibility of the MA part, in the following sense: \\(\\exists D(L) = \\sum_{k = -\\infty}^{+\\infty} D_k L^k\\) s.t. \\(D(L) \\Theta(L) = I\\), and therefore \\(\\eta_t = C^{-1}D(L)\\Phi(L)Y_t\\). This means that the structural shocks can be retrieved when all the past, current and future values of \\(Y_t\\) are observed. rules out cointegration (Hamilton 1994, p. 575). Indeed, consider the vector moving average (VMA) representation of the VARMA process, whose existence is guaranteed by Hp. 3: \\[ Y_t = \\underbrace{\\Phi(L)^{-1} \\Theta(L)}_{\\Psi(L)} \\varepsilon_t \\] A necessary condition for cointegration is the existence of a matrix \\(A\\in M_{n \\times n}\\) such that \\(A&#39; \\ \\Psi(1) = 0\\). This in turn implies that \\(\\det \\Psi(z) = \\det \\Phi(z)^{-1} \\det \\Theta(z) = 0\\) at \\(z \\in \\mathbb{C} \\text{ s.t. } |z| = 1\\). Since by Hp. 3\\(\\det \\Phi(z)^{-1} \\neq 0, \\ \\forall z \\in \\mathbb{C} \\ s.t. |z| \\geq 1\\), this implies that \\(\\det \\Theta(z)\\) has a unit root. Therefore, Hp. 4 is a sufficient condition to rule out the cointegration of the VARMA process. 1.2 Invertibility of the MA part A common approach in the estimation of VARMA models is to consider the truncated VAR representation of the process: \\(A(L) Y_t = \\varepsilon_t\\), where \\(A(L) = \\Theta(L)^{-1} \\Phi(L)\\). However, an implicit assumption behind this procedure is that \\(\\Theta(L)\\) is invertible in the past, i.e. \\(\\det\\Theta(z) \\neq 0, \\ \\forall z \\in \\mathbb{C} \\ s.t. \\ |z| &lt; 1\\). The following example illustrates the problem of ill-located roots of the determinental polynomial. Example 1.1 (Ill-located MA roots) Consider an ARMA(1,1) process: \\[ (1-\\phi L) y_t = (1 + \\theta L) \\varepsilon_t, \\quad |\\phi| &lt; 1, \\ |\\theta| &gt;1. \\] Since the AR component of the process is invertible, it admits an MA(\\(\\infty\\)) representation: \\[ \\begin{align}y_t &amp;= (1-\\phi L)^{-1}(1 + \\theta L) \\varepsilon_t = \\left(\\sum_{k=0}^{\\infty} \\phi^k L^k\\right) (1 + \\theta L) \\varepsilon_t = \\\\&amp;= \\sum_{k=0}^{\\infty} \\phi^k \\varepsilon_{t-k} + \\theta \\underbrace{\\sum_{k=1}^{\\infty} \\phi^k \\varepsilon_{t-k}}_{\\sum_{k=0}^{\\infty} \\phi^k \\varepsilon_{t-k}-\\varepsilon_t} \\\\\\implies y_t &amp;= (1+\\theta) \\sum_{k=0}^{\\infty} \\phi^k \\varepsilon_{t-k} - \\theta \\varepsilon_t\\end{align} \\]That is, the value observed at time \\(t\\) can be expressed as a linear combination of the present and past values of the shocks. On the contrary, the process does not admit an AR(\\(\\infty\\)) representation. Indeed, let \\(\\theta(L) = 1 + \\theta L\\). The equality holds if and only if it is \\(\\theta(z) = 1 + \\theta z\\), \\(\\forall z \\in \\mathbb{C}\\). The root of this polynomial is \\(z = -1/\\theta \\in \\mathbb{R}\\), which is smaller than one in modulus. This implies that the eigenvalue of the companion matrix \\(\\lambda\\) is such that \\(|\\lambda| &gt;1\\) (indeed, the companion matrix is the scalar \\(\\theta\\) itself). Therefore, the shock \\(\\varepsilon_t\\) would be an infinite sum of terms greater than one, which does not converge to any finite number. However, it is possible to obtain an alternative representation, which is autoregressive in the future values of \\(y_t\\). Start by rewriting \\(1+\\theta L\\) as \\(\\theta L (1 + \\frac{1}{\\theta} L^{-1})\\), where \\(L^{-1}\\) is the forward operator. The operator \\(1 + \\frac{1}{\\theta} L^{-1}\\) is invertible, and the process can be rewritten as: \\[ \\begin{align}\\theta L \\varepsilon_t &amp;= (1 + \\theta ^{-1} L ^{-1}) (1 - \\phi L ) y_t = \\\\&amp;= \\left( \\sum_{k = 0}^{\\infty} \\theta ^{-k} L ^{-k}\\right) (1-\\phi L) y_t \\end{align} \\] which, after some manipulations, becomes \\[ \\varepsilon_t = (1-\\phi)\\sum_{k = 1}^\\infty \\frac{1}{\\theta^k}y_{t+k} - \\phi y_t. \\] That is, the shocks of the process can be rewritten as a linear combination of the present and future values of \\(y_t\\). 1.3 Non-fundamentalness As the example shows, when the MA polynomial has some roots inside the unit circle there is an asymmetry in the information contained in the structural shocks and in the observed values. In this case, if we knew all the present and past shocks of the process we could retrieve its present and past values. On the contrary, if we knew all the present and past values of the process, we would still be unable to retrieve the past and present shocks. We can provide a rigorous explanation of this fact through the following Definition 1.1 Consider the process \\(x_t \\in \\mathbb{R}^n\\). Given its present and past observations\\(\\{x_t, x_{t-1}, x_{t-2},\\dots\\}\\), the information set is defined as \\[ \\mathcal{H}^t_x = \\overline{\\text{span}}(x_t, x_{t-1}, x_{t-2}, \\dots), \\] i.e. it is the set of all the possible linear combinations of the present and past values of \\(x_t\\). From the previous example, we can notice that: Knowing \\(\\{\\varepsilon_{t}, \\varepsilon_{t-1}, \\varepsilon_{t-2}, \\dots \\}\\) implies knowing also \\(\\{y_t, y_{t-1}, y_{t-2}, \\dots\\}\\) \\(\\implies\\) \\(\\mathcal{H}_y \\subseteq \\mathcal{H}_{\\varepsilon}\\) Knowing \\(\\{y_t, y_{t-1}, y_{t-2}, \\dots\\}\\) does not imply knowing also \\(\\{\\varepsilon_{t}, \\varepsilon_{t-1}, \\varepsilon_{t-2}, \\dots \\}\\) \\(\\implies\\) \\(\\mathcal{H}_\\varepsilon \\not\\subseteq \\mathcal{H}_{y}\\) Therefore, \\(\\mathcal{H}_y \\subset \\mathcal{H}_{\\varepsilon}\\). When this is the case, we say that the VARMA process is non-fundamental. Definition 1.2 The VARMA process \\(\\Phi(L) Y_t = \\Theta(L) \\varepsilon_t\\) is said to be non-fundamental when \\(\\mathcal{H}_Y \\subset \\mathcal{H}_{\\varepsilon}\\), or, analogously, when \\(\\exists z \\in \\mathbb{C}, |z|&lt;1, \\ s.t. \\ \\det(\\Theta(z)) = 0.\\) In that case, there exists at least one eigenvalue of the companion matrix of the MA part of the process that is larger than one in modulus. Non-fundamentalness may be a problem for VAR modeling in many practical situations. Indeed, if the VAR model approximates an underlying VARMA model, the identification of the true coefficients is impossible whenever the effect of some shocks intensifies after one or more time periods. Namely, non-fundamentalness arises whenever some entries of the \\(\\Theta\\) matrices are too big (see for instance Figure 6 in GMR2019 for the 2x2 case). This could be a relevant case in many economic application, where the strength of a shock could well increase after one or more period. Indeed, after its first outbreak, a shock might propagate in the system and, by affecting the expectations of the agents, it could reach its maximum effect with some delay. Therefore, the case of eigenvalues of the MA companion matrix greater than one in modulus cannot be simply discarded. The problem is made more significant by the fact that it is not possible to distinguish between a fundamental and a non-fundamental process using the standard techniques. This can be shown by exploiting the following relevant corollary to Theorem 2 in Lippi and Reichlin (1994): Theorem 1.1 Any non-fundamental VARMA process admits a fundamental representation that can be obtained by using Blaschke matrices. That is, given a non-fundamental \\(\\Phi(L) Y_t = \\Theta(L)\\varepsilon_t\\), there exists an invertible matrix \\(B(L)\\) with the property that \\(B(L)^{-1} = B^*(L^{-1})\\) such that \\[ \\Phi(L)Y_t = \\underbrace{\\Theta(L)B(L)^{-1}}_{\\tilde{\\Theta}(L)} \\underbrace{B(L)\\varepsilon_t}_{\\tilde{\\varepsilon}_t} \\] is fundamental, i.e. \\(\\det{\\tilde{\\Theta}(z)} \\neq 0, \\forall z \\in \\mathbb{C} \\ s.t. \\ |z|&lt;1\\), with \\(E[\\tilde{\\varepsilon}_t \\tilde{\\varepsilon}_s] = 0\\) but \\(\\tilde{\\varepsilon}_t\\) not independent on \\(\\tilde{\\varepsilon}_s\\). A consequence of Th. 1.1 is that standard methods for the estimation of VAR models do in general fail to identify the correct representation of the process. Indeed, if \\(\\Theta(L)\\) is not invertible, the VAR approximation becomes \\(\\tilde{A}(L) Y_t = \\tilde{\\varepsilon}_t\\), with \\(\\tilde{A}(L) = \\tilde{\\Theta}(L)^{-1}\\Phi(L)\\). Moreover, if we consider the process \\(\\tilde{Y}_t\\) such that \\(\\Phi(L) \\tilde{Y}_t = \\tilde{\\Theta}(L) \\tilde{C} \\eta_t\\), with \\(V(\\tilde{\\varepsilon}_t) = \\tilde{C}\\tilde{C}&#39;\\), the two processes \\(Y_t, \\tilde{Y}_t\\) have the same second-roder dynamic properties, namely the same autocovariance functions. This entails that all the methods that relies on distinguishing different second-order properties (like the Yule-Walker equations) fail to distinguish between \\(\\Theta(L)\\) and \\(\\tilde{\\Theta}(L)\\). However, the impulse response functions associated to shocks in \\(Y_t\\) and \\(\\tilde{Y}_t\\) are different. Example 1.2 (2nd order properties and IRF) Consider the MA(1) processes \\[ y_t = \\sigma \\eta_t - \\theta \\sigma \\eta_{t-1} \\\\ \\tilde{y}_t = \\sigma \\theta \\eta_t - \\sigma \\eta_{t-1} \\] with \\(E[\\eta_t] = 0, E[\\eta_t^2]= 1, \\theta\\sigma&gt;1\\). The two processes have the same autocovariance functions: \\[ \\begin{align} \\text{cov}(y_t, y_{t-h}) = \\text{cov}(\\tilde{y}_t, \\tilde{y}_{t-h}) = \\begin{cases} E[y_t^2] &amp;= E[\\tilde{y}_t^2] &amp;= \\sigma^2(1+\\theta^2) \\quad &amp; \\text{for } h = 0\\\\ E[y_t y_{t-1}] &amp;= E[\\tilde{y}_t \\tilde{y}_{t-1}] &amp;= -\\theta \\sigma^2 \\quad &amp; \\text{for } h = 1 \\\\ 0 &amp;&amp;&amp; \\text{for } h \\geq 2 \\end{cases} \\end{align} \\] However, \\(y_t\\) and \\(\\tilde{y}_t\\) respond differently to a shock in \\(\\eta_t\\): \\[ \\begin{align} \\frac{\\partial y_{t+h}}{\\partial \\eta_t} = \\begin{cases} \\sigma \\quad &amp; \\text{for } h = 0 \\\\ -\\theta \\sigma \\quad &amp; \\text{for } h = 1\\\\ 0 \\quad &amp; \\text{for } h \\geq 2 \\end{cases}; \\qquad \\frac{\\partial \\tilde{y}_{t+h}}{\\partial \\eta_t} = \\begin{cases} \\sigma \\theta \\quad &amp; \\text{for } h = 0 \\\\ -\\sigma \\quad &amp; \\text{for } h = 1\\\\ 0 \\quad &amp; \\text{for } h \\geq 2 \\end{cases} \\end{align} \\] The methods that try to distinguish fundamental and non-fundamental MA coefficients relying on the second-order properties of the process cannot work in this context. For this reason, GMR2019 proposes a new estimation procedure for SSVARMA models that relies on higher-order conditions for identifying the true structural shocks. Of course, the method is suitable for any distribution of the structural shocks with the only exception of the Gaussian distribution, whose higher-order moments are null whatever the mean and variance. "],["gmr-estimation-method.html", "Section 2 GMR estimation method 2.1 Maximum likelihood approach 2.2 Semi-parametric estimation: Two-stage 2SLS-GMM approach", " Section 2 GMR estimation method As we introduced in the previous Section, non-Gaussianity of the shocks is a necessary condition for identifiability. Hp.5 (Non-Gaussianity of the shocks). Each component of \\(\\eta_t\\) has a non-zero \\(r\\)-th cumulant with \\(r&gt;2\\) and at least one finite moment \\(s\\geq r\\). The following Theorem provides one of the main results in GMR2019. Theorem 2.1 (Observational equivalence of SSVARMA processes). Consider the SSVARMA processes \\(Y_t, \\tilde{Y}_t\\) defined by \\[ \\begin{align} \\Phi(L) Y_t &amp;= \\Theta(L) C \\eta_t\\\\ \\Phi(L) \\tilde{Y}_t &amp;= \\tilde{\\Theta}(L) \\tilde{C} \\tilde{\\eta}_t \\end{align} \\] Under Hp. 1-5, \\(Y_t\\) and \\(\\tilde{Y}_t\\) are observationally equivalent if and only if \\(\\Theta(L) = \\tilde{\\Theta}(L)\\), \\(C = \\tilde{C}\\) and the distribution of \\(\\eta_t\\) and \\(\\tilde{\\eta}_t\\) coincide. As a direct consequence of Th. 2.1, if processes characterized by Hp.1-5 differ for the MA part, then they can be distinguished by exploiting higher-order conditions. A last assumption that we make to assure global identifiability (i.e. to identify the coefficients with no ambiguity regarding their sign and order) is the following: Hp.6 (Global identifiability). The components of the first row of \\(C\\) are non-negative and in increasing order. The next two sections present the two methods developed by GMR2019. The first one is a parametric maximum-likelihood approach that requires to assume the distribution of the structural shocks in advance. The second one is a semi-parametric approach consisting in a two-step procedure. Even if it has the advantage of requiring no assumptions on the distribution of the shocks, it is less efficient than the ML procedure when the true distribution is known. Of course, both methods require that the distribution of the shocks is not Gaussian to exploit higher-moments for identification. 2.1 Maximum likelihood approach 2.1.1 MA(1) case Consider the simple MA(1) case first: \\(y_t = \\varepsilon_t + \\theta \\varepsilon_{t-1}\\), where \\(\\varepsilon_t\\) is i.i.d. over time, with a common non-Gaussian p.d.f. \\(g(\\varepsilon; \\gamma)\\), \\(\\gamma\\) being a vector of unknown parameters. We have to derive the log-likelihood of a sample of observed values \\((y_1, y_2, \\dots, y_T)\\), considering the three cases \\(|\\theta|&lt;1, \\ |\\theta| &gt;1, \\ |\\theta| = 1\\). To this end, we have to express the vector of shocks as a function of the observed values and of the parameters we want to estimate (in this case, \\(\\theta\\)). Since the precise expression of the shocks as a function of the past values of \\(y_t\\) would require a perfect knowledge of the infinite past before the shock, while we only observe a finite sample of the past, we need consider the truncated approximation of that expression. Therefore, the method is more precisily a truncated maximum-likelihood approach. Finally, we will substitute this expression into the p.d.f. of the shocks. Since the shocks are independent over time, their joint probability is the product of the probability of each draw. The maximum likelihood estimator will be the value of \\(\\theta\\) that maximizes the value of the likelihood function given the observed sample. \\(|\\theta|&lt;1\\): We can express \\(\\varepsilon_t\\) as a function of \\(\\theta\\) by exploiting standard invertibility: \\[ \\varepsilon_t(\\theta) = (1-\\theta L)^{-1} y_t = \\sum_{h = 0}^\\infty \\theta^h y_{t-h} = \\sum_{h = 0}^{t-1} \\theta^h y_{t-h} + \\theta^t \\underbrace{ \\sum_{h = 0}^{\\infty} \\theta^h y_{-h}}_{ = \\varepsilon_0(\\theta) \\text{ (unobserved)} } \\] The truncated log-likelihood expresses the probability of the observable approximation \\(\\varepsilon_t(\\theta) - \\varepsilon_0(\\theta)\\) as a function of the parameter \\(\\theta\\), for values \\(|\\theta|&lt;1\\): \\[ L_1(\\theta; y_1, \\dots, y_T, \\gamma) = \\log \\left(\\prod_{t=1}^Tg(\\varepsilon_t(\\theta) - \\varepsilon_0(\\theta); \\gamma)\\right) = \\sum_{t = 1}^T \\log g \\left(\\sum_{h = 0}^{t-1} \\theta^h y_{t-h}; \\gamma\\right), \\] where we have exploited the fact that the shocks are independent draws from the same distribution to express the joint probability \\(g_{\\varepsilon_1, \\dots, \\varepsilon_T}\\) as the product of the common marginal distribution \\(g\\). \\(\\theta&gt;1\\): for this case, we have to compute the likelihood that the observed sample came from a non-fundamental representation. As before, we have to express the shocks as a function of the observed values, by exploiting the generalized invertibility of the shocks in the future: \\[ y_t = (1+\\theta L) \\varepsilon_t = \\theta L(1+\\theta^{-1} L^{-1}) \\] from which \\[ \\begin{align} \\varepsilon_t(\\theta) &amp;= \\theta^{-1} L^{-1} (1+\\theta^{-1} L^{-1})^{-1} y_t = \\theta^{-1} L^{-1}(1+ \\theta^{-1} L^{-1} + \\theta^{-2} L^{-2} + \\dots ) y_t = = \\sum_{h = 1}^{\\infty} \\theta^{-h} y_{t+h} = \\\\ &amp; = \\sum_{h = 1}^{T-t} \\theta^{-h} y_{t+h} + \\theta^{-T-t} \\underbrace{ \\sum_{h = 1}^{\\infty} \\theta^{-h} y_{T+h}}_{\\varepsilon_T(\\theta) \\text{ (unobserved)}} \\end{align} \\] The truncated log-likelihood of \\(\\varepsilon_t(\\theta) - \\varepsilon_T(\\theta)\\) for \\(|\\theta|&gt;1\\) is: \\[ L_2(\\theta; y_1, \\dots, y_T, \\gamma) = \\sum_{t = 0}^{T-1} \\log\\left[ \\frac{1}{| \\theta|}g \\left(\\sum_{h = 1}^{T-t} \\theta^{-h} y_{t+h}; \\gamma\\right) \\right] \\] The factor \\(\\frac{1}{|\\theta|}\\) (which can be collected separately from the previous expression as \\(T \\log(1/|\\theta|)\\)) comes from the Jacobian equation that can be derived from the following well known theorem (Mood et al., p.211, Th. 15). Theorem 2.2 (P.d.f. of a transformed random vector) Consider \\[ \\begin{align} y_2 &amp; = f_1(\\varepsilon_1, \\varepsilon_2, \\dots, \\varepsilon_T) = \\varepsilon_{2} + \\theta \\varepsilon_{1}\\\\ y_3 &amp; = f_2(\\varepsilon_1, \\varepsilon_2, \\dots, \\varepsilon_T) = \\varepsilon_{3} + \\theta \\varepsilon_{2}\\\\ &amp; \\vdots \\\\ y_{T} &amp; = f_{T}(\\varepsilon_1, \\varepsilon_2, \\dots, \\varepsilon_T) = \\varepsilon_{T} + \\theta \\varepsilon_{T-1}\\\\ \\end{align} \\] (\\(f_1, \\dots, f_{T-1}\\) are one-to-one transformations of jointly continuous random variables with p.d.f. \\(g_{\\varepsilon_{2}, \\dots, \\varepsilon_{T}}(\\varepsilon_{2}, \\dots, \\varepsilon_{T})\\)). Consider also the inverse transformation \\[ \\begin{align} \\varepsilon_1(\\theta; y_1,\\dots, y_T) &amp;= f_1^{-1}(y_1,\\dots, y_T) = \\theta^{-1}y_2+ \\theta^{-2}y_3 +\\dots+ \\theta^{-(T-1)}y_T \\\\ \\varepsilon_2(\\theta; y_1,\\dots, y_T) &amp;= f_2^{-1}(y_1,\\dots, y_T) = 0 + \\theta^{-1}y_3+ \\theta^{-2}y_4 +\\dots+ \\theta^{-(T-2)}y_{T-1}\\\\ &amp; \\vdots \\\\ \\varepsilon_{T-1}(\\theta; y_1,\\dots, y_T) &amp;= f_{T-1}^{-1}(y_1,\\dots, y_T) = 0+0+\\dots+ 0+\\theta^{-1}y_T \\end{align} \\] Define \\(J\\) as the Jacobian of \\(f^{-1} = (f_1^{-1}, \\dots, f_{T-1}^{-1})\\): \\[ J = \\begin{bmatrix} \\frac{\\partial f_1^{-1}}{\\partial y_1} &amp; \\dots &amp; \\frac{\\partial f_1^{-1}}{\\partial y_{T-1}} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f_{T-1}^{-1}}{\\partial y_1} &amp; \\dots &amp; \\frac{\\partial f_{T-1}^{-1}}{\\partial y_{T-1}} \\end{bmatrix} = \\begin{bmatrix} \\theta^{-1} &amp; \\theta^{-2} &amp; \\dots &amp; \\theta^{-(T-1)} \\\\ 0 &amp; \\theta^{-1} &amp; \\dots &amp; \\theta^{-(T-2)} \\\\ \\vdots &amp;&amp;&amp; \\\\ 0 &amp; 0 &amp; \\dots &amp; \\theta^{-1} \\end{bmatrix} \\] Then the p.d.f. of the transformed vector \\(\\mathbf{y} = f(\\mathbf{\\varepsilon})\\) can be obtained as \\[ g_{y_1, \\dots, y_T} (.)= \\det J \\ g_{\\varepsilon_1, \\dots, \\varepsilon_T} (.) \\] The determinant of \\(J\\) is \\(\\det J = \\frac{1}{\\theta^{T}}\\). The truncated log-likelihood function is given by: \\[ L(\\theta; y_1, \\dots, y_T, \\gamma) = L_1(\\theta; y_1, \\dots, y_T, \\gamma) 1_{|\\theta|&lt;1} + L_2(\\theta; y_1, \\dots, y_T, \\gamma) 1_{|\\theta|\\geq1} \\] The function \\(L\\) is not continuous in \\(\\theta = 1\\). However, the exact likelihood function is continuous and differentiable. Indeed, it is given by: \\[ \\mathcal{L}(\\theta; y_1, \\dots, y_T, \\gamma) = \\log \\left[ \\int g(\\varepsilon_0 | y_1, \\dots, y_T; \\gamma) \\ d\\varepsilon \\right] \\propto \\log \\left[ \\int g(y_1, \\dots, y_T | \\varepsilon_0; \\gamma) g(\\varepsilon_0; \\gamma) \\ d \\varepsilon\\right], \\] which is in general differentiable, as the argument of the integral is a product of differentiable functions. Even though the exact likelihood cannot be directly computed, many properties of the truncated likelihood are asymptotically equivalent to those of the exact likelihood, and proving its properties is therefore useful. Proposition 2.1 (Consistency of the truncated maximum likelihood estimator, Proposition 2 in GMR2019) Let \\(\\Lambda_0\\) be the true vector of parameters of a SSVARMA(p,q) model, and let \\(\\hat{\\Lambda}_T, \\hat{\\Lambda}_T^u\\) denote respectively the truncated and untruncated maximum likelihood estimators of \\(\\Lambda_0\\). Under Hp. 1-6 and under some further regularity conditions, \\[ \\hat{\\Lambda}_T, \\hat{\\Lambda}_T^u \\ \\xrightarrow{a.s.} \\ \\Lambda_0 \\] Moreover, the two estimators are asymptotically normal. 2.1.2 SSVARMA(p,1) case The multivariate case requires some methematical expedients. Let us consider first the SSVARMA(p,1) case: the general SSVAMRA(p,q) estimation will then be a naural extension. The process is: \\[ \\Phi(L) Y_t = \\varepsilon_t + \\Theta \\varepsilon_{t-1} \\] where the errors are linear combinations of the i.i.d. structural shocks \\(\\eta_t\\) with independent components: \\(\\varepsilon_t = C \\eta_t\\), with \\(E[\\eta_{t}] = 0\\), \\(V[\\eta_{t}] = 1\\). The p.d.f. of the errors is \\(g(\\varepsilon_t; C, \\gamma)\\). As a first step, consider the Schur decomposition of matrix \\(\\Theta\\): \\[ \\Theta = A&#39;_{\\Theta} U_{\\Theta} A_{\\Theta}, \\] where \\(A_{\\Theta}\\) is an orthogonal matrix (meaning that \\(A^{-1}_{\\Theta} = A&#39;_{\\Theta}\\)) and \\(U_{\\Theta}\\) is an upper block-triangular matrix, whose diagonal blocks cointains the eigenvalues of \\(\\Theta\\). The \\(1 \\times 1\\) blocks are the real eigenvalues, the \\(2 \\times 2\\) blocks contains the complex eigenvalues \\(\\lambda, \\overline{\\lambda}\\) in the form \\[ U_{k\\Theta} = \\begin{bmatrix} \\text{Re}(\\lambda) &amp; \\text{Im}(\\lambda) \\\\ - \\text{Im}(\\lambda) &amp; \\text{Re}(\\lambda) \\end{bmatrix}, \\] \\(n_k \\in \\{1,2\\}\\) will denote the dimension of the matrix \\(U_{k\\Theta}\\). We can left-multiply \\(\\Phi(L) Y_t = \\varepsilon_t + \\Theta \\varepsilon_{t-1}\\) by \\(A_{\\Theta}&#39;\\) to get a VMA(1) representation of the process. Define \\(W_t = A_{\\Theta}&#39;\\Phi(L) Y_t\\) and \\(\\varepsilon^*_t = A_{\\Theta}&#39; \\varepsilon_t\\) to get \\[ W_t = \\varepsilon_t^* - U_\\Theta \\varepsilon_{t-1}^* \\] Without loss of generality, we assume that the block-diagonal elements \\(U_{k\\Theta}\\) have eigenvalues larger than one for \\(k \\in \\{1, \\dots ,s\\}\\) and eigenvalues smaller than one for \\(k \\in \\{s+1, \\dots , K\\}\\). We can also define \\(\\varepsilon_t^{(1)}, \\varepsilon_t^{(2)}\\) such that \\(\\varepsilon_t = [\\varepsilon_t&#39;^{(1)}, \\varepsilon_t&#39;^{(2)}]&#39;\\), where \\(\\varepsilon_t^{(1)}\\) has length equal to the non-fundamentalness order \\(m = n_1 + \\dots + n_s\\) and \\(\\varepsilon_t^{(2)}\\) has length equal to \\(n-m\\). Similarly we also write \\(W_t = [W_t&#39;^{(1)}, W_t&#39;^{(2)}]&#39;\\). We have: \\[ \\begin{bmatrix} \\varepsilon_t^{(1)} \\\\ \\varepsilon_t^{(2)} \\end{bmatrix} = \\begin{bmatrix} W_t^{(1)} \\\\ W_t^{(2)} \\end{bmatrix} + \\begin{bmatrix} U_\\Theta^{(1)} &amp; U_\\Theta^{(12)} \\\\ 0 &amp; U_\\Theta^{(2)} \\end{bmatrix} \\begin{bmatrix} \\varepsilon_{t-1}^{(1)} \\\\ \\varepsilon_{t-2}^{(2)} \\end{bmatrix} \\] The shocks can now be expressed as a function of \\(W_t\\) (which is in turn a function of the data and the parameters). Moreover, we can distinguish different expression for the errors associated with eigenvalues smaller or bigger than one. In particular, in analogy with the MA(1) case, we can express \\(\\varepsilon_t^{(2)}\\) as a linear combination of the present and past values of the data, and \\(\\varepsilon_t^{(1)}\\) as a linear combination of the present and future values of the data: \\[ \\begin{align} \\varepsilon_t^{*(2)} &amp; = W_t^{(2)} + U_\\Theta^{(2)}W_{t-1}^{(2)} + [U_\\Theta^{(2)}]^2 W_{t-2}^{(2)} + \\dots + [U_\\Theta^{(2)}]^{t-1} W_{1}^{(2)} + \\underbrace{[U_\\Theta^{(2)}]^t \\varepsilon^{*(2)}_0}_{\\text{unobserved}}\\\\ \\varepsilon_t^{*(1)} &amp; = \\underbrace{[U_\\Theta^{(1)}]^{-(T-t)} \\varepsilon_T^{*(1)}}_{\\text{unobserved}} - [U_\\Theta^{(1)}]^{-1} W_{t+1}^{*(1)} - [U_\\Theta^{(1)}]^{-2} W_{t+2}^{*(1)} - \\dots - [U_\\Theta^{(1)}]^{-(T-t)} W_{T}^{*(1)} + \\\\ &amp; - \\underbrace{[U_{\\Theta}^{(1)}]^{-1}U_{\\Theta}^{(12)} \\varepsilon_t^{*(2)}}_{\\text{truncated}} - \\dots - \\underbrace{[U_{\\Theta}^{(1)}]^{-(T-t)}U_{\\Theta}^{(12)} \\varepsilon_{T-1}^{*(2)}}_{\\text{truncated}} \\end{align} \\] Keeping in mind that \\(\\varepsilon_t = A_{\\Theta} \\varepsilon_t^{*}\\), we can now write the truncated log-likelihood function. Let \\(\\Lambda = \\{\\Phi_1, \\dots, \\Phi_p, \\Theta, C, \\gamma\\}\\) be the vector of parameters to be estimated and \\(\\lambda_i(\\Theta)\\) the \\(i\\)-th eigenvalue of \\(\\Theta\\). The truncated versions of \\(\\varepsilon_t^{*(1)}, \\varepsilon_t^{*(2)}\\), which depend only on the model parameters and on observed values, are: \\[ \\begin{align} \\varepsilon_t^{*(2)}|_{truncated} &amp;= W_t^{(2)} + U_\\Theta^{(2)}W_{t-1}^{(2)} + [U_\\Theta^{(2)}]^2 W_{t-2}^{(2)} + \\dots + [U_\\Theta^{(2)}]^{t-1} W_{1}^{(2)} \\\\ \\varepsilon_t^{*(1)}|_{truncated} &amp;= [U_\\Theta^{(1)}]^{-1} W_{t+1}^{*(1)} - [U_\\Theta^{(1)}]^{-2} W_{t+2}^{*(1)} - \\dots - [U_\\Theta^{(1)}]^{-(T-t)} W_{T}^{*(1)} + \\\\ &amp; - [U_{\\Theta}^{(1)}]^{-1}U_{\\Theta}^{(12)} (W_t^{(2)} + U_\\Theta^{(2)}W_{t-1}^{(2)} + \\dots + [U_\\Theta^{(2)}]^{t-1} W_{1}^{(2)})- \\dots + \\\\ &amp; - [U_{\\Theta}^{(1)}]^{-(T-t)}U_{\\Theta}^{(12)} (W_{T-1}^{(2)} + U_\\Theta^{(2)}W_{T-2}^{(2)} + \\dots + [U_\\Theta^{(2)}]^{T-2} W_{1}^{(2)} ) \\end{align} \\] By expressing \\(\\varepsilon^*_t\\) as a function of the observed data and the model parameters, we can set up the Jacobian equation to get \\[ L_{truncated}(\\Lambda) = -T \\sum_{i = 1}^{n} \\log |\\lambda_i(\\Theta)| 1_{|\\lambda_i(\\Theta)|&gt;1} + \\sum_{t = 1}^T \\log g\\left[A_{\\Theta} \\begin{pmatrix} \\varepsilon_t^{*(1)}|_{truncated} \\\\ \\varepsilon_t^{*(2)}|_{truncated} \\end{pmatrix} ; C, \\gamma \\right] \\] Again, Prop. 2.1 applies and the truncated ML estimator \\(\\hat{\\Lambda} = \\arg\\max_\\Lambda L_{truncated}(\\Lambda)\\) is a consistent and asymptotically normal estimator of the true \\(\\Lambda\\). 2.1.3 SVARMA(p,q) case The general case where \\(\\Theta(L)\\) is of order \\(q&gt;1\\) can be reduced to the previous one. Define \\[ \\begin{align} \\tilde{\\varepsilon}_t &amp;= [\\varepsilon_t&#39;, \\dots, \\varepsilon_{t-q+1}&#39;]&#39; \\\\ \\tilde{Y}_t &amp;= [Y_t&#39;, 0_{1 \\times(n-1)q}]&#39; \\\\ \\tilde{\\Phi_k} &amp;= \\mathbf{uu}&#39; \\otimes \\Phi_k, \\quad \\text{where} \\ \\ \\mathbf{u} = [1,0,\\dots,0]&#39;\\\\ \\\\ \\tilde{\\Theta} &amp;= \\begin{bmatrix} \\Theta_1 &amp; \\Theta_2 &amp; \\dots &amp; \\Theta_{q-1} &amp; \\Theta_q \\\\ I &amp; 0 &amp; \\dots &amp; 0 &amp; 0 \\\\ 0 &amp; I &amp; \\dots &amp; 0 &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; I &amp; 0 \\\\ \\end{bmatrix} \\end{align} \\] where the eigenvalues of \\(\\tilde{\\Theta}\\) are the reciprocal of the roots of \\(\\det \\Theta(z)\\). Then, the process can be rewritten as \\[ \\tilde{\\Phi}(L) \\tilde{Y}_t = \\tilde{\\varepsilon}_t - \\tilde{\\Theta}\\tilde{\\varepsilon}_{t-1} \\] which reduces to the previous case (even though the process is not a VARMA(p,1), because \\(\\tilde{\\varepsilon}_t\\) is not a white noise). It is useful to stress the role that non-Gaussianity plays in this method. Indeed, it is not immediately clear why we should not choose the p.d.f. of the Normal distribution as our \\(g(.)\\). The problem is that with a Gaussian p.d.f., the limiting likelihood function (if the infinite time series were observed) would assume the same value both in the true value of the paramater and in its fundamenral representation. In the practical case of a finite sample, the likelihood would be maximum in one point, but it would be impossible to say whether the estimate approximates the true value of the parameters or their fundamental representation. The following example clarifies this case. Example 2.1 (The role of non-Gaussianity) Consider the MA(1) process \\(y_t = \\varepsilon_t - \\theta \\varepsilon_{t-1}\\), with \\(\\varepsilon_t \\sim N(0, \\sigma^2)\\), i.i.d. and mutually independent. Consider the value of the likelihood function if the whole time series \\(\\{y_t\\}_{t=-\\infty}^{+\\infty}\\) were observed: \\[ L_\\infty(\\theta; y_1, \\dots, y_T, \\gamma) = 1_{|\\theta|&lt;1} \\tilde{L}_1((\\theta; y_1, \\dots, y_T, \\gamma) + 1_{|\\theta|\\geq1} \\tilde{L}_2((\\theta; y_1, \\dots, y_T, \\gamma) \\] where, in the Gaussian case: \\[ \\begin{align} \\tilde{L}_1(\\theta; y_1, \\dots, y_T, \\gamma) &amp;= E_0 \\log g \\left(\\sum_{h = 0}^{+\\infty} \\theta^h y_{t-h}; \\gamma\\right) =\\\\ &amp;=E_0\\log \\left[ \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left( -\\frac{1}{2}\\frac{(\\sum_{h = 0}^{+\\infty} \\theta^h y_{t-h})^2}{\\sigma^2} \\right) \\right] = \\\\ &amp; = -\\frac{1}{2} \\log(2\\pi) -\\frac{1}{2} \\log \\sigma^2 - \\frac{1}{2\\sigma^2} E_0 \\left[\\left(\\sum_{h = 0}^{+\\infty} \\theta^h y_{t-h}\\right)^2\\right]\\\\ \\tilde{L}_2(\\theta; y_1, \\dots, y_T, \\gamma) &amp;= E_0 \\log \\frac{1}{|\\theta|} g \\left(-\\sum_{h = 0}^{+\\infty} \\frac{1}{\\theta^{h+1}} y_{t+h+1}; \\gamma\\right) = \\\\ &amp; = -\\frac{1}{2} \\log(2 \\pi) - \\frac{1}{2} \\log(\\theta^2\\sigma^2) - \\frac{1}{2\\theta^2 \\sigma^2} E_0 \\left[\\left(\\sum_{h = 0}^{+\\infty} \\frac{1}{\\theta^h} y_{t+h+1}\\right)^2\\right] \\end{align} \\] The asymptotic log-likelihood \\(L_\\infty\\) reaches its minimum at both \\((\\theta, \\sigma^2)\\) and \\((\\frac{1}{\\theta}, \\theta^2 \\sigma^2)\\). Therefore, \\(\\theta\\) and \\(\\sigma\\) are not identified, and the finite sample ML estimation cannot distinguish between the fundamental and non-fundamental representation of the same process. 2.2 Semi-parametric estimation: Two-stage 2SLS-GMM approach The main drawback of maximum likelihood estimation is that we need to choose a priori a probability density function for the structural shocks, which is in general unknown. We could think of some methods to choose which distribution of the shocks best fits the data. For instance, we could compare the likelihood of the same model with different p.d.f.s and choose the functional form which gives the likelihood function with highest maximum. However, for the \\(q = 1\\) case, GMR2019 propose a different approach, namely a semi-parametric estimation method which makes no prior assumption on the distribution of the shocks. Consider a SVARMA(p,1) process: \\[ \\begin{equation} Y_t = \\Phi_1 Y_{t-1} + \\dots + \\Phi_p Y_{t-p} + C\\eta_t + C \\Theta \\eta_{t-1} \\tag{2.1} \\end{equation} \\] The procedure consists of two steps. First, we use two-stage least squares (2SLS) to estimate \\(\\Phi_1, \\dots, \\Phi_p\\) and \\(Z_t = C\\eta_t + C \\Theta \\eta_{t-1}\\) as \\(\\hat{Z}_t = Y_t - (\\hat\\Phi_1 Y_{t-1} + \\dots + \\hat\\Phi_p Y_{t-p})\\). Indeed, estimating directly Eq. (2.1) would lead to biased estimates, because the error terms are correlated with \\(Y_{t-1}\\): \\(E[Y_{t-1} Z_t] = E[(\\Phi_1 Y_{t-2} + \\dots + \\Phi_{p} Y_{t-p-1} + C\\eta_{t-1} + C \\Theta \\eta_{t-2})( C\\eta_t + C \\Theta \\eta_{t-1}) = C^2\\Theta E[\\eta_{t-1}^2] \\neq 0\\). On the contrary, we can use \\(Y_{t-2}, \\dots, Y_{t-1-k}, k \\geq p\\) as instruments, whose exogeneity is guaranteed by \\(E[ C\\eta_t + C \\Theta \\eta_{t-1}| Y_{t-2}, \\dots, Y_{t-1-k}] = 0\\), to obtain consistent estimates \\(\\Phi_1\\) and, in turn, of \\(\\hat{Z}_t\\). Note that the condition \\(k\\geq p\\) assures that the first-stage regression of the TSLS estimator has at least one regressor which is not included in the main equation, thus avoiding multicollinearity. The second step exploits moment restrictions on the structural shocks to estimate the mixing matrix \\(C\\) and the matrix of the MA coefficients \\(\\Theta\\) from \\(\\hat{Z}_t\\), in a pure MA framework (note that, as pointed out by GMR2019 themselves, if \\(\\Theta_1 = 0\\) then \\(C\\) can be directly be estimated via ICA). To this aim, the assumption of non-Gaussianity is crucial, since the moment restrictions regard moments higher than the second. To obtain the moment restrictions, we consider the pairwise log-Laplace transform of \\((Z_t, Z_{t-1})\\), i.e. their joint cumulant generating function (c.g.f.). This would allow to impose the conditions on the cumulants, that uniquely identify the distribution of the shocks. For any \\(u,v \\in \\mathbb{R}^n\\), the c.g.f. is \\[ \\begin{align} \\log E[\\exp(u&#39;Z_t + v&#39;Z_{t-1})] &amp;= \\log E[\\exp\\left(u&#39;(C\\eta_t + (\\Theta C) \\eta_{t-1}) + v&#39;(C \\eta_{t-1} + (\\Theta C) \\eta_{t-2}) \\right)] = \\\\ &amp;= \\log E[\\exp(u&#39;(C\\eta_t + (\\Theta C) \\eta_{t-1}) \\exp(v&#39;(C\\eta_{t-1} + (\\Theta C) \\eta_{t-2})] = \\\\ &amp;= \\log \\left( E[\\exp(u&#39;C \\eta_t)] E[\\exp(u&#39;(\\Theta C) \\eta_{t-1})] E[\\exp(v&#39;C \\eta_{t-1})] E[\\exp(v&#39;(\\Theta C) \\eta_{t-2})] \\right) \\tag{2.2} \\end{align} \\] where the last equality comes from the fact that if two generic random variables \\(X, Y\\) are independent (like \\(\\eta_t, \\eta_{t-1}, \\eta_{t-2}\\)), then \\(E[e^{t(X+Y)}] = E[e^{tX}]E[e^{tY}]\\). The c.g.f. in Eq. (2.2) can be expanded as a McLaurin power series of the form \\[ K(w) = \\sum_{n= 1}^\\infty \\kappa_n\\frac{w^n}{n!} \\] where \\(\\kappa_n\\) is the \\(n\\)-th cumulant of the distribution. To apply the moment conditions, we expand the series up to the fourth degree. Since \\(E[\\eta_j] = 0, E[\\eta^2_j] = 1\\), we have \\[ K_{\\eta_j}(w) = \\frac{w^2}{2} + \\frac{w^3}{6}\\kappa_{3j} + \\frac{w^4}{24}\\kappa_{4j} \\] By further algebraic manipulations, we can get the four moment conditions that hold for any couple \\((u,v)\\). \\[ \\begin{align} E[(u&#39;Z_t + v&#39; Z_{t-1})^2] &amp;= \\sum_{j= 1}^n [(u&#39;C_j)^2 + (u&#39;(\\Theta C)_j + v&#39;C_j)^2 + (v&#39;(\\Theta C)_j)^2]\\\\ E[(u&#39;Z_t + v&#39; Z_{t-1})^3] &amp;= \\sum_{j= 1}^n \\kappa_{3j}[(u&#39;C_j)^3 + (u&#39;(\\Theta C)_j + v&#39;C_j)^3 + (v&#39;(\\Theta C)_j)^3] \\\\ E[(u&#39;Z_t + v&#39; Z_{t-1})^4] &amp;= \\sum_{j= 1}^n \\kappa_{4j}[(u&#39;C_j)^4 + (u&#39;(\\Theta C)_j + v&#39;C_j)^4 + (v&#39;(\\Theta C)_j)^4] + \\\\ &amp; + 3\\left(\\sum_{j= 1}^n [(u&#39;C_j)^2 + (u&#39;(\\Theta C)_j + v&#39;C_j)^2 + (v&#39;(\\Theta C)_j)^2]\\right)^2 \\tag{2.3} \\end{align} \\] Therefore, the set of moment restrictions is of the form \\[ E[h(Z_t, Z_{t-1}); \\beta] = 0, \\quad \\text{with } \\beta = [\\text{vec}C&#39;, \\text{vec}(\\Theta C)&#39;, \\kappa_{31}, \\dots, \\kappa_{3n}, \\kappa_{41}, \\dots, \\kappa_{4n}] \\] where \\(h(.)\\) is defined as in Eqs. (2.3). Since the number of parameters to be estimated in \\(\\beta\\) is \\(2n^2+2n\\), the order condition is \\(r\\geq 2n^2+2n\\). Therefore, by exploiting the analogy principle between the population and the sample moments, we can obtain the estimator for \\(\\beta\\) as \\[ \\hat\\beta = \\arg\\min_\\beta \\frac{1}{T}\\sum_{t=1}^Th(\\hat{Z}_t, \\hat{Z}_{t-1}; \\beta) \\] The following example clarifies the procedure in the MA(1) case. Example 2.2 (Moment method in the MA(1) case) In the case of a MA(1), \\(\\theta\\) can be easily identified if the distribution of the shocks is skewed. Indeed: \\[ \\begin{align} E[y_t y_{t-1}^2] &amp;= E[(\\varepsilon_{t}-\\theta \\varepsilon_{t-1})(\\varepsilon_{t-1}-\\theta \\varepsilon_{t-2})^2] = -\\theta E[\\varepsilon_{t}^3] \\\\ E[y_t^2 y_{t-1}] &amp;= E[(\\varepsilon_{t}-\\theta \\varepsilon_{t-1})^2(\\varepsilon_{t-1}-\\theta \\varepsilon_{t-2})] = \\theta^2 E[\\varepsilon_{t}^3] \\end{align} \\] Therefore, whenever \\(E[\\varepsilon_{t}^3] \\neq 0\\), the parameter is identified as \\[ \\theta = - \\frac{E[y_t^2 y_{t-1}]}{E[y_t y_{t-1}^2]} \\] and can be consistently estimated as \\(\\hat\\theta = - \\frac{\\sum_{t=1}^T y_t y_t^2}{\\sum_{t=1}^T y_t^2 y_t}\\). "]]
